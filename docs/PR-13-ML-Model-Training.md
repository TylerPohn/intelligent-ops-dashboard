# PR-13: Train Custom ML Model for InfiniBand Prediction

## Overview

**Status:** Not Started
**Priority:** High
**Complexity:** Very High
**Estimated Effort:** 6-8 weeks
**Dependencies:** PR-12 (AI Integration)

**Current State:** Using pre-trained Claude 3.5 Haiku via Bedrock API inference ($0.000375/insight).

**Goal:** Train custom ML model on InfiniBand-specific data for improved accuracy, lower latency, and reduced operational costs (~90% cost reduction to $0.00004/insight).

**Model Type:** Custom classifier/regressor using Amazon SageMaker or Bedrock custom model training.

---

## Architecture Overview

### Current (PR-12): Pre-trained LLM Inference
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Metrics   â”‚â”€â”€â”€â”€â”€â–ºâ”‚   Bedrock    â”‚â”€â”€â”€â”€â”€â–ºâ”‚   Insight   â”‚
â”‚   (Input)   â”‚      â”‚  Claude 3.5  â”‚      â”‚  (Output)   â”‚
â”‚             â”‚      â”‚    Haiku     â”‚      â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ (Pre-trained)â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â””â”€â–º $0.000375 per inference
                           â””â”€â–º 500ms latency
                           â””â”€â–º Generic model
```

### Target: Custom Trained ML Model
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ML Model Training & Deployment Pipeline                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Data    â”‚      â”‚  Model Training  â”‚      â”‚ Model Deployment â”‚
â”‚   Generation     â”‚â”€â”€â”€â”€â”€â–ºâ”‚   SageMaker      â”‚â”€â”€â”€â”€â”€â–ºâ”‚   Inference      â”‚
â”‚                  â”‚      â”‚   AutoML/Custom  â”‚      â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                         â”‚                         â”‚
         â”‚                         â”‚                         â”‚
         â–¼                         â–¼                         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 100K+    â”‚           â”‚  Optimized   â”‚        â”‚  Endpoint       â”‚
   â”‚ Samples  â”‚           â”‚  Model       â”‚        â”‚  <50ms latency  â”‚
   â”‚ Labeled  â”‚           â”‚  Artifacts   â”‚        â”‚  $0.00004/call  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Complete ML Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Phase 1: Training Data Generation                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Historical Production Metrics   â”‚
                    â”‚  (DynamoDB: 30 days history)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Data Collection Lambda          â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ Query historical metrics   â”‚  â”‚
                    â”‚  â”‚ Extract features           â”‚  â”‚
                    â”‚  â”‚ Join with outcomes         â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Expert Labeling System         â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ Manual Review UI           â”‚  â”‚
                    â”‚  â”‚ Auto-label via rules       â”‚  â”‚
                    â”‚  â”‚ Claude-assisted labeling   â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     S3 Training Dataset          â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ train.csv (70K samples)    â”‚  â”‚
                    â”‚  â”‚ validation.csv (15K)       â”‚  â”‚
                    â”‚  â”‚ test.csv (15K)            â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Phase 2: Model Training                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3 Training Data   â”‚      â”‚  SageMaker Training  â”‚      â”‚  Model Registry  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ Features     â”‚â”€â”€â”€â”¼â”€â”€â”€â”€â”€â–ºâ”‚  â”‚ XGBoost        â”‚  â”‚      â”‚  â”‚ Best Model â”‚  â”‚
â”‚   â”‚ - latency    â”‚   â”‚      â”‚  â”‚ or             â”‚â”€â”€â”¼â”€â”€â”€â”€â”€â–ºâ”‚  â”‚ Artifacts  â”‚  â”‚
â”‚   â”‚ - utilizationâ”‚   â”‚      â”‚  â”‚ Neural Network â”‚  â”‚      â”‚  â”‚ Metrics    â”‚  â”‚
â”‚   â”‚ - error_rate â”‚   â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚   â”‚ - bandwidth  â”‚   â”‚      â”‚                      â”‚      â”‚                  â”‚
â”‚   â”‚ - qp_errors  â”‚   â”‚      â”‚  Hyperparameter Tuningâ”‚     â”‚                  â”‚
â”‚   â”‚ - deltas     â”‚   â”‚      â”‚  Cross-validation     â”‚     â”‚                  â”‚
â”‚   â”‚ (15 features)â”‚   â”‚      â”‚  Early stopping       â”‚     â”‚                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ Labels       â”‚   â”‚
â”‚   â”‚ - pred_type  â”‚   â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   â”‚ - risk_score â”‚   â”‚              â”‚  Training Metrics Dashboard  â”‚
â”‚   â”‚ - severity   â”‚   â”‚              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚              â”‚  â”‚ Accuracy: 94.2%        â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”‚ Precision: 91.8%       â”‚  â”‚
                                       â”‚  â”‚ Recall: 93.5%          â”‚  â”‚
                                       â”‚  â”‚ F1-Score: 92.6%        â”‚  â”‚
                                       â”‚  â”‚ AUC-ROC: 0.96          â”‚  â”‚
                                       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Phase 3: Model Deployment                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Registry  â”‚      â”‚ SageMaker Endpoint â”‚      â”‚  AI Lambda (Updated) â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Model v1.3 â”‚â”€â”€â”¼â”€â”€â”€â”€â”€â–ºâ”‚  â”‚ Real-time    â”‚â—„â”€â”¼â”€â”€â”€â”€â”€â”€â”‚  â”‚ Call Endpoint  â”‚  â”‚
â”‚  â”‚ Accuracy:  â”‚  â”‚      â”‚  â”‚ Inference    â”‚  â”‚      â”‚  â”‚ (not Bedrock)  â”‚  â”‚
â”‚  â”‚   94.2%    â”‚  â”‚      â”‚  â”‚ <50ms        â”‚  â”‚      â”‚  â”‚                â”‚  â”‚
â”‚  â”‚ Size: 5MB  â”‚  â”‚      â”‚  â”‚ Auto-scaling â”‚  â”‚      â”‚  â”‚ Fallback to    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚  â”‚ Bedrock if failâ”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚                    â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                          â”‚  Cost per 1K calls: â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚  $0.04 (vs $0.375)  â”‚
                          â”‚  93% cost savings!  â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Phase 4: Production Integration                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Process Lambda â”‚      â”‚   AI Lambda      â”‚      â”‚    DynamoDB              â”‚
â”‚  (Unchanged)   â”‚â”€â”€â”€â”€â”€â–ºâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                â”‚      â”‚  â”‚ Feature    â”‚  â”‚      â”‚  â”‚ Insights (labeled) â”‚  â”‚
â”‚ Detect         â”‚      â”‚  â”‚ Engineeringâ”‚  â”‚      â”‚  â”‚ - prediction_type  â”‚  â”‚
â”‚ Anomalies      â”‚      â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚  â”‚ - risk_score       â”‚  â”‚
â”‚                â”‚      â”‚        â”‚          â”‚      â”‚  â”‚ - model_version    â”‚  â”‚
â”‚ Trigger AI     â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚  â”‚ - confidence       â”‚  â”‚
â”‚                â”‚      â”‚  â”‚ SageMaker  â”‚  â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                â”‚      â”‚  â”‚ Endpoint   â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                â”‚      â”‚  â”‚ (Primary)  â”‚  â”‚
â”‚                â”‚      â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                â”‚      â”‚        â”‚ fail?   â”‚      â”‚  Continuous Learning     â”‚
â”‚                â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                â”‚      â”‚  â”‚  Bedrock   â”‚  â”‚      â”‚  â”‚ Collect feedback   â”‚  â”‚
â”‚                â”‚      â”‚  â”‚ (Fallback) â”‚  â”‚      â”‚  â”‚ Retrain monthly    â”‚  â”‚
â”‚                â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚  â”‚ A/B test versions  â”‚  â”‚
â”‚                â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Legend:
â”€â”€â”€â”€â”€â–º Data Flow
â”Œâ”€â”€â”€â”€â” Component/Service
â”‚    â”‚ Processing Step
```

---

## Detailed Training Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     End-to-End Training Workflow                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Week 1-2: Data Collection & Labeling
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 1.1     â”‚  Query DynamoDB for 30 days of production metrics
  â”‚ Data Export  â”‚  â†’ 50K+ raw metric records
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â†’ Export to S3 in Parquet format
         â”‚
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 1.2     â”‚  Join metrics with observed outcomes:
  â”‚ Join Outcomesâ”‚  - Did alert fire within 5 min?
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - What was actual severity?
         â”‚          - What action was taken?
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 1.3     â”‚  Feature Engineering:
  â”‚ Feature Eng. â”‚  - Rolling averages (5m, 15m, 1h)
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - Delta from baseline (%, absolute)
         â”‚          - Rate of change (derivative)
         â–¼          - Time-based features (hour, day_of_week)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  - Interaction features (latency Ã— utilization)
  â”‚ Step 1.4     â”‚
  â”‚ Auto-Labelingâ”‚  Rule-based labeling (80% coverage):
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - Utilization >90% â†’ resource_saturation
         â”‚          - Latency >3x baseline â†’ performance_degradation
         â–¼          - Error rate >5% â†’ connection_instability
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 1.5     â”‚  Manual expert review (20% ambiguous):
  â”‚ Expert Reviewâ”‚  - Web UI for network engineers
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - Claude-assisted suggestions
         â”‚          - Quality control sampling
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 1.6     â”‚  Final Dataset:
  â”‚ Train/Val/   â”‚  - 70K training samples
  â”‚ Test Split   â”‚  - 15K validation samples
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - 15K test samples (held out)
         â”‚          - Stratified split (balanced classes)
         â–¼
    S3 Bucket: s3://iops-ml-training/datasets/v1/


Week 3-4: Model Training & Tuning
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 2.1     â”‚  Baseline Model (XGBoost):
  â”‚ Train XGBoostâ”‚  - Default hyperparameters
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - 10-fold cross-validation
         â”‚          - Baseline accuracy: ~85%
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 2.2     â”‚  Hyperparameter Tuning:
  â”‚ HPO Search   â”‚  - SageMaker Automatic Model Tuning
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - 50 training jobs (parallel)
         â”‚          - Optimize for F1-score
         â–¼          - Best params found
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 2.3     â”‚  Advanced Models:
  â”‚ Try Neural   â”‚  - Multi-layer perceptron (MLP)
  â”‚ Networks     â”‚  - LSTM for time-series features
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - Ensemble methods
         â”‚          - Compare performance
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 2.4     â”‚  Model Selection:
  â”‚ Select Best  â”‚  - XGBoost winner (accuracy: 94.2%)
  â”‚ Model        â”‚  - Lightweight (5MB)
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - Fast inference (<50ms)
         â”‚          - Interpretable (feature importance)
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 2.5     â”‚  Evaluation Metrics:
  â”‚ Thorough     â”‚  - Accuracy: 94.2%
  â”‚ Evaluation   â”‚  - Precision: 91.8% (few false positives)
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - Recall: 93.5% (catch most issues)
         â”‚          - F1-Score: 92.6%
         â–¼          - AUC-ROC: 0.96 (excellent discrimination)
    Model Registry: s3://iops-ml-models/xgboost-v1.3/


Week 5: Deployment & Integration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 3.1     â”‚  Create SageMaker Endpoint:
  â”‚ Deploy       â”‚  - Real-time inference
  â”‚ Endpoint     â”‚  - ml.t3.medium instance (cost-effective)
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - Auto-scaling (1-5 instances)
         â”‚          - Model latency: <50ms
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 3.2     â”‚  Update AI Lambda:
  â”‚ Update Lambdaâ”‚  - Call SageMaker instead of Bedrock
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - Feature preparation code
         â”‚          - Fallback to Bedrock on error
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 3.3     â”‚  Canary Deployment:
  â”‚ Canary Test  â”‚  - 10% traffic to ML model
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - 90% traffic to Bedrock (baseline)
         â”‚          - Compare predictions
         â–¼          - Monitor accuracy
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 3.4     â”‚  Gradual Rollout:
  â”‚ Ramp to 100% â”‚  - 10% â†’ 25% â†’ 50% â†’ 100%
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - Monitor each stage
         â”‚          - Validate cost savings
         â–¼
    Production: 100% traffic to ML model


Week 6-8: Monitoring & Continuous Learning
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 4.1     â”‚  Collect Prediction Feedback:
  â”‚ Feedback Loopâ”‚  - User corrections (false positives)
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - Actual outcomes (did issue occur?)
         â”‚          - Store in training dataset
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 4.2     â”‚  Drift Detection:
  â”‚ Monitor Driftâ”‚  - Feature distribution changes
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - Prediction quality degradation
         â”‚          - Alert if accuracy drops >2%
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Step 4.3     â”‚  Monthly Retraining:
  â”‚ Retrain      â”‚  - Include new labeled data
  â”‚ Schedule     â”‚  - Automated pipeline
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - A/B test new version
         â”‚          - Deploy if improved
         â–¼
    Continuous Improvement Loop
```

---

## Swarm Coordination Plan

### Multi-Agent Training Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Claude-Flow Swarm for ML Training Pipeline                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Queen Agent (Orchestrator)            â”‚
         â”‚  - Coordinate all training phases      â”‚
         â”‚  - Monitor progress                    â”‚
         â”‚  - Report status                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                                        â”‚
         â–¼                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Collector     â”‚                  â”‚  Labeling Agent    â”‚
â”‚  Agent             â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Query DynamoDBâ”‚  â”‚                  â”‚  â”‚ Auto-label   â”‚  â”‚
â”‚  â”‚ Export to S3  â”‚  â”‚                  â”‚  â”‚ Rule-based   â”‚  â”‚
â”‚  â”‚ Format data   â”‚  â”‚                  â”‚  â”‚ Expert UI    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                          â”‚
         â”‚                                          â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Feature Engineer Agent â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
         â”‚  â”‚ Create features  â”‚  â”‚
         â”‚  â”‚ Transform data   â”‚  â”‚
         â”‚  â”‚ Validate schema  â”‚  â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Training Agent        â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
         â”‚  â”‚ Launch SageMaker â”‚  â”‚
         â”‚  â”‚ Hyperparameter   â”‚  â”‚
         â”‚  â”‚ tuning           â”‚  â”‚
         â”‚  â”‚ Monitor progress â”‚  â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Evaluation Agent      â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
         â”‚  â”‚ Test model       â”‚  â”‚
         â”‚  â”‚ Generate metrics â”‚  â”‚
         â”‚  â”‚ Create reports   â”‚  â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Deployment Agent      â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
         â”‚  â”‚ Create endpoint  â”‚  â”‚
         â”‚  â”‚ Update Lambda    â”‚  â”‚
         â”‚  â”‚ Canary test      â”‚  â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Swarm Agent Roles

**Queen Agent (Orchestrator)**
- Initialize swarm topology (hierarchical)
- Assign tasks to specialist agents
- Monitor progress via memory coordination
- Handle errors and retries
- Generate final report

**Data Collector Agent**
- Query DynamoDB for historical metrics
- Export to S3 in Parquet format
- Validate data quality
- Report statistics

**Labeling Agent**
- Apply rule-based auto-labeling
- Generate expert review queue
- Coordinate with SMEs via UI
- Validate label quality

**Feature Engineer Agent**
- Create time-series features
- Calculate deltas and derivatives
- Normalize and scale features
- Generate feature importance analysis

**Training Agent**
- Launch SageMaker training jobs
- Configure hyperparameter tuning
- Monitor training progress
- Handle failures and retries

**Evaluation Agent**
- Run model on test set
- Calculate performance metrics
- Generate confusion matrix
- Create evaluation report

**Deployment Agent**
- Create SageMaker endpoint
- Update AI Lambda code
- Deploy with canary strategy
- Monitor production metrics

---

## Work Breakdown

## 1. Training Data Generation

### 1.1 Historical Data Collection

**File:** `scripts/collect-training-data.py`

```python
#!/usr/bin/env python3
"""
Collect historical metrics from DynamoDB for ML training.

Usage:
  python scripts/collect-training-data.py --days 30 --output s3://iops-ml-training/raw/
"""

import boto3
from datetime import datetime, timedelta
import pandas as pd
from typing import List, Dict
import pyarrow.parquet as pq

dynamodb = boto3.resource('dynamodb', region_name='us-east-2')
s3 = boto3.client('s3')
table = dynamodb.Table('iops-dashboard-metrics')

def collect_metrics(days: int = 30) -> pd.DataFrame:
    """
    Query DynamoDB for historical metrics.

    Returns DataFrame with columns:
    - timestamp
    - stream_id
    - latency_p99
    - utilization
    - error_rate
    - bandwidth_gbps
    - qp_errors
    - [other metrics]
    """
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)

    metrics = []

    # Query with pagination
    response = table.query(
        IndexName='EntityTypeIndex',
        KeyConditionExpression='entity_type = :type AND #ts BETWEEN :start AND :end',
        ExpressionAttributeNames={'#ts': 'timestamp'},
        ExpressionAttributeValues={
            ':type': 'metric',
            ':start': start_date.isoformat(),
            ':end': end_date.isoformat(),
        },
    )

    metrics.extend(response['Items'])

    # Handle pagination
    while 'LastEvaluatedKey' in response:
        response = table.query(
            IndexName='EntityTypeIndex',
            KeyConditionExpression='entity_type = :type AND #ts BETWEEN :start AND :end',
            ExpressionAttributeNames={'#ts': 'timestamp'},
            ExpressionAttributeValues={
                ':type': 'metric',
                ':start': start_date.isoformat(),
                ':end': end_date.isoformat(),
            },
            ExclusiveStartKey=response['LastEvaluatedKey'],
        )
        metrics.extend(response['Items'])

    df = pd.DataFrame(metrics)
    print(f"Collected {len(df)} metric records")

    return df

def join_with_outcomes(metrics_df: pd.DataFrame) -> pd.DataFrame:
    """
    Join metrics with observed outcomes (alerts that fired).

    For each metric, look forward 5 minutes to see if:
    - Alert was generated
    - What was the prediction type
    - What was the actual severity
    """
    # Query insights that occurred after each metric
    insights_response = table.query(
        IndexName='EntityTypeIndex',
        KeyConditionExpression='entity_type = :type',
        ExpressionAttributeValues={':type': 'insight'},
    )

    insights_df = pd.DataFrame(insights_response['Items'])

    # Convert timestamps to datetime
    metrics_df['timestamp'] = pd.to_datetime(metrics_df['timestamp'])
    insights_df['timestamp'] = pd.to_datetime(insights_df['timestamp'])

    # Merge on stream_id and time window (within 5 minutes)
    merged = pd.merge_asof(
        metrics_df.sort_values('timestamp'),
        insights_df.sort_values('timestamp'),
        on='timestamp',
        by='stream_id',
        direction='forward',
        tolerance=pd.Timedelta('5 minutes'),
    )

    return merged

def save_to_s3(df: pd.DataFrame, s3_path: str) -> None:
    """Save DataFrame to S3 as Parquet."""
    df.to_parquet(s3_path, compression='snappy', index=False)
    print(f"Saved {len(df)} rows to {s3_path}")

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--days', type=int, default=30)
    parser.add_argument('--output', type=str, required=True)
    args = parser.parse_args()

    # Collect data
    metrics_df = collect_metrics(days=args.days)

    # Join with outcomes
    labeled_df = join_with_outcomes(metrics_df)

    # Save to S3
    save_to_s3(labeled_df, args.output)
```

### 1.2 Auto-Labeling System

**File:** `scripts/auto-label-data.py`

```python
#!/usr/bin/env python3
"""
Automatically label training data using rule-based heuristics.

Covers ~80% of cases, leaving 20% for expert review.
"""

import pandas as pd
from typing import Dict, Tuple

PREDICTION_TYPES = [
    'performance_degradation',
    'anomaly_detected',
    'resource_saturation',
    'connection_instability',
    'packet_loss_detected',
    'latency_spike',
    'bandwidth_throttling',
    'qp_error_rate_high',
    'memory_registration_failure',
    'rdma_timeout',
]

def auto_label_row(row: pd.Series, baseline: Dict[str, float]) -> Tuple[str, int, float]:
    """
    Auto-label a metric row based on rules.

    Returns:
        (prediction_type, risk_score, confidence)
    """

    # Rule 1: Resource Saturation
    if row['utilization'] > 90:
        risk = min(100, 70 + (row['utilization'] - 90) * 3)
        return 'resource_saturation', int(risk), 0.95

    # Rule 2: High QP Error Rate
    if row['qp_errors'] > 100:
        risk = min(100, 60 + row['qp_errors'] / 10)
        return 'qp_error_rate_high', int(risk), 0.90

    # Rule 3: RDMA Timeout (error rate + high latency)
    if row['error_rate'] > 0.05 and row['latency_p99'] > baseline['latency'] * 4:
        return 'rdma_timeout', 95, 0.92

    # Rule 4: Performance Degradation
    latency_increase = (row['latency_p99'] - baseline['latency']) / baseline['latency']
    if latency_increase > 0.5:  # 50% increase
        risk = min(100, 60 + latency_increase * 60)
        return 'performance_degradation', int(risk), 0.85

    # Rule 5: Packet Loss
    if row['error_rate'] > 0.02:
        risk = min(100, 50 + row['error_rate'] * 1000)
        return 'packet_loss_detected', int(risk), 0.88

    # Rule 6: Latency Spike
    if latency_increase > 0.3 and latency_increase <= 0.5:
        return 'latency_spike', 70, 0.80

    # Rule 7: Bandwidth Throttling
    bandwidth_ratio = row['bandwidth_gbps'] / baseline['bandwidth']
    if bandwidth_ratio < 0.7:  # 30% reduction
        risk = int(60 + (1 - bandwidth_ratio) * 50)
        return 'bandwidth_throttling', risk, 0.75

    # Rule 8: Connection Instability (frequent errors)
    if row['error_rate'] > 0.01 and row['error_rate'] <= 0.02:
        return 'connection_instability', 65, 0.82

    # Rule 9: General Anomaly (if nothing else matches but clearly abnormal)
    if any([
        row['utilization'] > 80,
        latency_increase > 0.2,
        row['error_rate'] > 0.005,
    ]):
        return 'anomaly_detected', 55, 0.65

    # No clear label - mark for expert review
    return None, 0, 0.0

def label_dataset(df: pd.DataFrame) -> pd.DataFrame:
    """
    Apply auto-labeling to entire dataset.

    Adds columns:
    - prediction_type
    - risk_score
    - label_confidence
    - needs_review (bool)
    """

    # Calculate baselines per stream
    baselines = df.groupby('stream_id').agg({
        'latency_p99': 'median',
        'utilization': 'median',
        'bandwidth_gbps': 'median',
    }).to_dict('index')

    labels = []

    for idx, row in df.iterrows():
        stream_baseline = baselines.get(row['stream_id'], {
            'latency_p99': 5,  # Default baseline
            'utilization': 60,
            'bandwidth_gbps': 100,
        })

        pred_type, risk, confidence = auto_label_row(row, stream_baseline)

        labels.append({
            'prediction_type': pred_type,
            'risk_score': risk,
            'label_confidence': confidence,
            'needs_review': pred_type is None or confidence < 0.75,
        })

    labels_df = pd.DataFrame(labels)
    df = pd.concat([df, labels_df], axis=1)

    coverage = (df['prediction_type'].notna()).sum() / len(df)
    print(f"Auto-labeling coverage: {coverage*100:.1f}%")
    print(f"Rows needing expert review: {df['needs_review'].sum()}")

    return df

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--input', type=str, required=True)
    parser.add_argument('--output', type=str, required=True)
    args = parser.parse_args()

    # Load unlabeled data
    df = pd.read_parquet(args.input)

    # Apply auto-labeling
    labeled_df = label_dataset(df)

    # Save labeled data
    labeled_df.to_parquet(args.output, compression='snappy', index=False)
```

### 1.3 Expert Review UI (Streamlit)

**File:** `scripts/expert-review-ui.py`

```python
#!/usr/bin/env python3
"""
Streamlit UI for expert review of ambiguous training samples.

Usage:
  streamlit run scripts/expert-review-ui.py
"""

import streamlit as st
import pandas as pd
import boto3
from datetime import datetime

s3 = boto3.client('s3')

st.set_page_config(page_title="ML Training Data Labeling", layout="wide")

st.title("ğŸ·ï¸ InfiniBand ML Training Data Labeling")

# Load data needing review
@st.cache_data
def load_review_queue():
    df = pd.read_parquet('s3://iops-ml-training/labeled/needs_review.parquet')
    return df[df['needs_review'] == True].reset_index(drop=True)

review_df = load_review_queue()

st.sidebar.header("Progress")
st.sidebar.metric("Total Samples", len(review_df))
st.sidebar.metric("Reviewed", 0)  # Track via session state

# Sample selector
idx = st.number_input("Sample Index", min_value=0, max_value=len(review_df)-1, value=0)

sample = review_df.iloc[idx]

st.header(f"Sample {idx+1}/{len(review_df)}")

# Display metric values
col1, col2, col3 = st.columns(3)

with col1:
    st.metric("Stream ID", sample['stream_id'])
    st.metric("Timestamp", sample['timestamp'])
    st.metric("Latency P99", f"{sample['latency_p99']:.2f} Î¼s")

with col2:
    st.metric("Utilization", f"{sample['utilization']:.1f}%")
    st.metric("Error Rate", f"{sample['error_rate']:.3f}%")
    st.metric("Bandwidth", f"{sample['bandwidth_gbps']:.1f} Gbps")

with col3:
    st.metric("QP Errors", int(sample['qp_errors']))
    st.metric("Auto Label", sample['prediction_type'] or "None")
    st.metric("Confidence", f"{sample['label_confidence']:.2f}")

# Claude-assisted suggestion
st.subheader("ğŸ¤– Claude's Suggestion")

suggestion = f"""
Based on the metrics:
- Latency: {sample['latency_p99']:.2f} Î¼s
- Utilization: {sample['utilization']:.1f}%
- Error Rate: {sample['error_rate']:.3f}%

Suggested label: **{sample['prediction_type'] or 'anomaly_detected'}**
Risk score: **{sample['risk_score'] or 60}**
"""

st.info(suggestion)

# Expert labeling form
st.subheader("Expert Label")

prediction_types = [
    'performance_degradation',
    'anomaly_detected',
    'resource_saturation',
    'connection_instability',
    'packet_loss_detected',
    'latency_spike',
    'bandwidth_throttling',
    'qp_error_rate_high',
    'memory_registration_failure',
    'rdma_timeout',
    'false_positive',  # No issue
]

with st.form("label_form"):
    expert_label = st.selectbox(
        "Prediction Type",
        options=prediction_types,
        index=prediction_types.index(sample['prediction_type']) if sample['prediction_type'] in prediction_types else 0
    )

    expert_risk = st.slider("Risk Score", min_value=0, max_value=100, value=int(sample['risk_score'] or 50))

    expert_notes = st.text_area("Notes (optional)")

    submit = st.form_submit_button("Submit Label")

    if submit:
        # Save expert label
        review_df.loc[idx, 'prediction_type'] = expert_label
        review_df.loc[idx, 'risk_score'] = expert_risk
        review_df.loc[idx, 'expert_notes'] = expert_notes
        review_df.loc[idx, 'reviewed_by'] = 'expert'
        review_df.loc[idx, 'reviewed_at'] = datetime.now().isoformat()

        # Update S3
        review_df.to_parquet('s3://iops-ml-training/labeled/expert_reviewed.parquet')

        st.success(f"Labeled sample {idx+1} as {expert_label}")
        st.balloons()
```

### 1.4 Feature Engineering

**File:** `scripts/engineer-features.py`

```python
#!/usr/bin/env python3
"""
Feature engineering for ML model training.

Creates derived features from raw metrics:
- Rolling averages (5m, 15m, 1h)
- Deltas from baseline
- Rate of change
- Time-based features
- Interaction features
"""

import pandas as pd
import numpy as np
from typing import List

def create_rolling_features(df: pd.DataFrame) -> pd.DataFrame:
    """Create rolling window features."""

    df = df.sort_values(['stream_id', 'timestamp'])

    # Rolling averages (5 minutes)
    df['latency_p99_5m_avg'] = df.groupby('stream_id')['latency_p99'].transform(
        lambda x: x.rolling(window=5, min_periods=1).mean()
    )

    df['utilization_5m_avg'] = df.groupby('stream_id')['utilization'].transform(
        lambda x: x.rolling(window=5, min_periods=1).mean()
    )

    # Rolling standard deviation (volatility)
    df['latency_p99_5m_std'] = df.groupby('stream_id')['latency_p99'].transform(
        lambda x: x.rolling(window=5, min_periods=1).std()
    )

    # 15-minute and 1-hour windows
    df['latency_p99_15m_avg'] = df.groupby('stream_id')['latency_p99'].transform(
        lambda x: x.rolling(window=15, min_periods=1).mean()
    )

    df['latency_p99_1h_avg'] = df.groupby('stream_id')['latency_p99'].transform(
        lambda x: x.rolling(window=60, min_periods=1).mean()
    )

    return df

def create_baseline_deltas(df: pd.DataFrame) -> pd.DataFrame:
    """Calculate deltas from baseline (median per stream)."""

    # Calculate per-stream baselines
    baselines = df.groupby('stream_id').agg({
        'latency_p99': 'median',
        'utilization': 'median',
        'error_rate': 'median',
        'bandwidth_gbps': 'median',
    }).add_suffix('_baseline')

    df = df.merge(baselines, left_on='stream_id', right_index=True)

    # Percentage deltas
    df['latency_p99_delta_pct'] = (
        (df['latency_p99'] - df['latency_p99_baseline']) / df['latency_p99_baseline'] * 100
    )

    df['utilization_delta_pct'] = (
        df['utilization'] - df['utilization_baseline']
    )

    df['bandwidth_delta_pct'] = (
        (df['bandwidth_gbps'] - df['bandwidth_gbps_baseline']) / df['bandwidth_gbps_baseline'] * 100
    )

    # Absolute deltas
    df['latency_p99_delta_abs'] = df['latency_p99'] - df['latency_p99_baseline']

    return df

def create_rate_of_change(df: pd.DataFrame) -> pd.DataFrame:
    """Calculate rate of change (derivative)."""

    df = df.sort_values(['stream_id', 'timestamp'])

    # Latency rate of change
    df['latency_p99_rate'] = df.groupby('stream_id')['latency_p99'].diff()

    # Utilization rate of change
    df['utilization_rate'] = df.groupby('stream_id')['utilization'].diff()

    # Second derivative (acceleration)
    df['latency_p99_accel'] = df.groupby('stream_id')['latency_p99_rate'].diff()

    return df

def create_time_features(df: pd.DataFrame) -> pd.DataFrame:
    """Extract time-based features."""

    df['timestamp'] = pd.to_datetime(df['timestamp'])

    df['hour'] = df['timestamp'].dt.hour
    df['day_of_week'] = df['timestamp'].dt.dayofweek
    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
    df['is_business_hours'] = df['hour'].between(8, 18).astype(int)

    return df

def create_interaction_features(df: pd.DataFrame) -> pd.DataFrame:
    """Create interaction features."""

    # High latency + high utilization = severe issue
    df['latency_x_utilization'] = df['latency_p99'] * df['utilization'] / 100

    # Error rate weighted by bandwidth
    df['error_x_bandwidth'] = df['error_rate'] * df['bandwidth_gbps']

    # Composite health score (inverse)
    df['health_score'] = 100 - (
        (df['utilization'] * 0.4) +
        (df['latency_p99_delta_pct'].clip(0, 100) * 0.3) +
        (df['error_rate'] * 1000 * 0.3)
    ).clip(0, 100)

    return df

def engineer_all_features(df: pd.DataFrame) -> pd.DataFrame:
    """Apply all feature engineering steps."""

    print("Creating rolling features...")
    df = create_rolling_features(df)

    print("Creating baseline deltas...")
    df = create_baseline_deltas(df)

    print("Creating rate of change features...")
    df = create_rate_of_change(df)

    print("Creating time features...")
    df = create_time_features(df)

    print("Creating interaction features...")
    df = create_interaction_features(df)

    # Drop NaN rows (from rolling windows)
    df = df.dropna()

    print(f"Feature engineering complete. {len(df)} rows, {len(df.columns)} features")

    return df

# Feature list for model training
FEATURE_COLUMNS = [
    # Raw metrics
    'latency_p99',
    'utilization',
    'error_rate',
    'bandwidth_gbps',
    'qp_errors',

    # Rolling features
    'latency_p99_5m_avg',
    'latency_p99_5m_std',
    'latency_p99_15m_avg',
    'latency_p99_1h_avg',
    'utilization_5m_avg',

    # Baseline deltas
    'latency_p99_delta_pct',
    'latency_p99_delta_abs',
    'utilization_delta_pct',
    'bandwidth_delta_pct',

    # Rate of change
    'latency_p99_rate',
    'latency_p99_accel',
    'utilization_rate',

    # Time features
    'hour',
    'day_of_week',
    'is_weekend',
    'is_business_hours',

    # Interaction features
    'latency_x_utilization',
    'error_x_bandwidth',
    'health_score',
]

TARGET_COLUMN = 'prediction_type'
RISK_SCORE_COLUMN = 'risk_score'

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--input', type=str, required=True)
    parser.add_argument('--output', type=str, required=True)
    args = parser.parse_args()

    # Load labeled data
    df = pd.read_parquet(args.input)

    # Engineer features
    df = engineer_all_features(df)

    # Save training-ready dataset
    df.to_parquet(args.output, compression='snappy', index=False)
```

---

## 2. Model Training (SageMaker)

### 2.1 Training Script (XGBoost)

**File:** `training/train_xgboost.py`

```python
#!/usr/bin/env python3
"""
SageMaker training script for XGBoost classifier.

Trains multi-class classifier for prediction_type.
Trains separate regressor for risk_score.
"""

import argparse
import os
import pandas as pd
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    confusion_matrix,
    classification_report,
)
import joblib
import json

def train_classifier(X_train, y_train, X_val, y_val, args):
    """Train XGBoost classifier for prediction type."""

    # Encode labels
    le = LabelEncoder()
    y_train_encoded = le.fit_transform(y_train)
    y_val_encoded = le.transform(y_val)

    # Train XGBoost
    dtrain = xgb.DMatrix(X_train, label=y_train_encoded)
    dval = xgb.DMatrix(X_val, label=y_val_encoded)

    params = {
        'objective': 'multi:softmax',
        'num_class': len(le.classes_),
        'max_depth': args.max_depth,
        'learning_rate': args.learning_rate,
        'n_estimators': args.n_estimators,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'eval_metric': 'mlogloss',
    }

    model = xgb.train(
        params,
        dtrain,
        num_boost_round=args.n_estimators,
        evals=[(dtrain, 'train'), (dval, 'val')],
        early_stopping_rounds=10,
        verbose_eval=10,
    )

    # Evaluate
    y_pred = model.predict(dval)
    accuracy = accuracy_score(y_val_encoded, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_val_encoded, y_pred, average='weighted'
    )

    print(f"\n=== Classifier Metrics ===")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")

    print("\n=== Classification Report ===")
    print(classification_report(y_val_encoded, y_pred, target_names=le.classes_))

    return model, le

def train_regressor(X_train, y_train, X_val, y_val, args):
    """Train XGBoost regressor for risk score."""

    dtrain = xgb.DMatrix(X_train, label=y_train)
    dval = xgb.DMatrix(X_val, label=y_val)

    params = {
        'objective': 'reg:squarederror',
        'max_depth': args.max_depth,
        'learning_rate': args.learning_rate,
        'n_estimators': args.n_estimators,
        'eval_metric': 'rmse',
    }

    model = xgb.train(
        params,
        dtrain,
        num_boost_round=args.n_estimators,
        evals=[(dtrain, 'train'), (dval, 'val')],
        early_stopping_rounds=10,
        verbose_eval=10,
    )

    # Evaluate
    y_pred = model.predict(dval)
    from sklearn.metrics import mean_squared_error, r2_score
    rmse = mean_squared_error(y_val, y_pred, squared=False)
    r2 = r2_score(y_val, y_pred)

    print(f"\n=== Regressor Metrics ===")
    print(f"RMSE: {rmse:.4f}")
    print(f"RÂ²: {r2:.4f}")

    return model

if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    # Hyperparameters
    parser.add_argument('--max_depth', type=int, default=6)
    parser.add_argument('--learning_rate', type=float, default=0.1)
    parser.add_argument('--n_estimators', type=int, default=100)

    # SageMaker environment
    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
    parser.add_argument('--validation', type=str, default=os.environ.get('SM_CHANNEL_VALIDATION'))
    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))

    args = parser.parse_args()

    # Load data
    train_df = pd.read_parquet(f"{args.train}/train.parquet")
    val_df = pd.read_parquet(f"{args.validation}/validation.parquet")

    # Feature columns (from feature engineering)
    from scripts.engineer_features import FEATURE_COLUMNS

    X_train = train_df[FEATURE_COLUMNS]
    y_train_class = train_df['prediction_type']
    y_train_risk = train_df['risk_score']

    X_val = val_df[FEATURE_COLUMNS]
    y_val_class = val_df['prediction_type']
    y_val_risk = val_df['risk_score']

    # Train classifier
    classifier, label_encoder = train_classifier(
        X_train, y_train_class, X_val, y_val_class, args
    )

    # Train regressor
    regressor = train_regressor(
        X_train, y_train_risk, X_val, y_val_risk, args
    )

    # Save models
    classifier.save_model(f"{args.model_dir}/classifier.xgb")
    regressor.save_model(f"{args.model_dir}/regressor.xgb")
    joblib.dump(label_encoder, f"{args.model_dir}/label_encoder.pkl")

    # Save feature names
    with open(f"{args.model_dir}/feature_names.json", 'w') as f:
        json.dump(FEATURE_COLUMNS, f)

    print("\nâœ… Training complete!")
```

### 2.2 SageMaker Training Job (CDK)

**File:** `cdk/lib/ml-training-stack.ts`

```typescript
import * as cdk from 'aws-cdk-lib';
import * as sagemaker from 'aws-cdk-lib/aws-sagemaker';
import * as s3 from 'aws-cdk-lib/aws-s3';
import * as iam from 'aws-cdk-lib/aws-iam';

export class MLTrainingStack extends cdk.Stack {
  constructor(scope: cdk.App, id: string, props?: cdk.StackProps) {
    super(scope, id, props);

    // S3 bucket for training data and models
    const mlBucket = new s3.Bucket(this, 'MLBucket', {
      bucketName: 'iops-ml-training',
      versioned: true,
      encryption: s3.BucketEncryption.S3_MANAGED,
    });

    // SageMaker execution role
    const sageMakerRole = new iam.Role(this, 'SageMakerRole', {
      assumedBy: new iam.ServicePrincipal('sagemaker.amazonaws.com'),
      managedPolicies: [
        iam.ManagedPolicy.fromAwsManagedPolicyName('AmazonSageMakerFullAccess'),
      ],
    });

    mlBucket.grantReadWrite(sageMakerRole);

    // Training job (will be triggered via boto3)
    const trainingJobName = 'iops-xgboost-training';

    // Note: CDK doesn't have L2 construct for training jobs
    // Use boto3 to launch training job from Lambda/script
  }
}
```

### 2.3 Launch Training Job (Script)

**File:** `scripts/launch-training.py`

```python
#!/usr/bin/env python3
"""
Launch SageMaker training job.

Usage:
  python scripts/launch-training.py --job-name iops-xgboost-v1
"""

import boto3
import time
from datetime import datetime

sagemaker = boto3.client('sagemaker', region_name='us-east-2')

def launch_training_job(job_name: str):
    """Launch SageMaker training job."""

    response = sagemaker.create_training_job(
        TrainingJobName=job_name,
        RoleArn='arn:aws:iam::971422717446:role/SageMakerRole',
        AlgorithmSpecification={
            'TrainingImage': '683313688378.dkr.ecr.us-east-2.amazonaws.com/sagemaker-xgboost:1.5-1',
            'TrainingInputMode': 'File',
        },
        InputDataConfig=[
            {
                'ChannelName': 'train',
                'DataSource': {
                    'S3DataSource': {
                        'S3DataType': 'S3Prefix',
                        'S3Uri': 's3://iops-ml-training/datasets/v1/train/',
                        'S3DataDistributionType': 'FullyReplicated',
                    },
                },
            },
            {
                'ChannelName': 'validation',
                'DataSource': {
                    'S3DataSource': {
                        'S3DataType': 'S3Prefix',
                        'S3Uri': 's3://iops-ml-training/datasets/v1/validation/',
                        'S3DataDistributionType': 'FullyReplicated',
                    },
                },
            },
        ],
        OutputDataConfig={
            'S3OutputPath': 's3://iops-ml-training/models/',
        },
        ResourceConfig={
            'InstanceType': 'ml.m5.xlarge',
            'InstanceCount': 1,
            'VolumeSizeInGB': 30,
        },
        StoppingCondition={
            'MaxRuntimeInSeconds': 3600,  # 1 hour
        },
        HyperParameters={
            'max_depth': '6',
            'learning_rate': '0.1',
            'n_estimators': '100',
        },
    )

    print(f"Training job launched: {job_name}")
    print(f"ARN: {response['TrainingJobArn']}")

    # Monitor progress
    while True:
        status = sagemaker.describe_training_job(TrainingJobName=job_name)
        state = status['TrainingJobStatus']

        print(f"Status: {state}")

        if state in ['Completed', 'Failed', 'Stopped']:
            break

        time.sleep(30)

    if state == 'Completed':
        print("âœ… Training completed successfully!")
        print(f"Model artifacts: {status['ModelArtifacts']['S3ModelArtifacts']}")
    else:
        print(f"âŒ Training failed: {status.get('FailureReason')}")

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--job-name', type=str, required=True)
    args = parser.parse_args()

    launch_training_job(args.job_name)
```

---

## 3. Model Deployment

### 3.1 Create SageMaker Endpoint

**File:** `scripts/deploy-endpoint.py`

```python
#!/usr/bin/env python3
"""
Deploy trained model to SageMaker real-time endpoint.

Usage:
  python scripts/deploy-endpoint.py --model-path s3://iops-ml-training/models/xgboost-v1.3/
"""

import boto3
import time

sagemaker = boto3.client('sagemaker', region_name='us-east-2')

def create_model(model_name: str, model_path: str):
    """Create SageMaker model."""

    response = sagemaker.create_model(
        ModelName=model_name,
        PrimaryContainer={
            'Image': '683313688378.dkr.ecr.us-east-2.amazonaws.com/sagemaker-xgboost:1.5-1',
            'Mode': 'SingleModel',
            'ModelDataUrl': model_path,
        },
        ExecutionRoleArn='arn:aws:iam::971422717446:role/SageMakerRole',
    )

    print(f"Model created: {model_name}")

def create_endpoint_config(config_name: str, model_name: str):
    """Create endpoint configuration."""

    response = sagemaker.create_endpoint_config(
        EndpointConfigName=config_name,
        ProductionVariants=[
            {
                'VariantName': 'AllTraffic',
                'ModelName': model_name,
                'InstanceType': 'ml.t3.medium',  # Cost-effective
                'InitialInstanceCount': 1,
                'InitialVariantWeight': 1,
            },
        ],
    )

    print(f"Endpoint config created: {config_name}")

def create_endpoint(endpoint_name: str, config_name: str):
    """Create SageMaker endpoint."""

    response = sagemaker.create_endpoint(
        EndpointName=endpoint_name,
        EndpointConfigName=config_name,
    )

    print(f"Endpoint creating: {endpoint_name}")

    # Monitor progress
    while True:
        status = sagemaker.describe_endpoint(EndpointName=endpoint_name)
        state = status['EndpointStatus']

        print(f"Status: {state}")

        if state in ['InService', 'Failed']:
            break

        time.sleep(30)

    if state == 'InService':
        print("âœ… Endpoint ready for inference!")
        print(f"Endpoint ARN: {status['EndpointArn']}")
    else:
        print(f"âŒ Endpoint creation failed: {status.get('FailureReason')}")

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--model-path', type=str, required=True)
    parser.add_argument('--endpoint-name', type=str, default='iops-xgboost-endpoint')
    args = parser.parse_args()

    model_name = f"{args.endpoint_name}-model"
    config_name = f"{args.endpoint_name}-config"

    create_model(model_name, args.model_path)
    create_endpoint_config(config_name, model_name)
    create_endpoint(args.endpoint_name, config_name)
```

### 3.2 Update AI Lambda for SageMaker

**File:** `lambda/ai/handler.py` (Updated)

```python
import boto3
import json
import os
from typing import Dict, Any

sagemaker_runtime = boto3.client('sagemaker-runtime', region_name='us-east-2')
bedrock_client = boto3.client('bedrock-runtime', region_name='us-east-2')

SAGEMAKER_ENDPOINT = os.environ.get('SAGEMAKER_ENDPOINT', 'iops-xgboost-endpoint')
USE_SAGEMAKER = os.environ.get('USE_SAGEMAKER', 'true').lower() == 'true'

def prepare_features(metric: dict) -> list:
    """
    Prepare feature vector for SageMaker inference.

    Must match training feature order exactly.
    """
    from scripts.engineer_features import FEATURE_COLUMNS

    # Calculate derived features (same as training)
    features = []

    # Raw metrics
    features.append(metric.get('latency_p99', 0))
    features.append(metric.get('utilization', 0))
    features.append(metric.get('error_rate', 0))
    features.append(metric.get('bandwidth_gbps', 0))
    features.append(metric.get('qp_errors', 0))

    # Rolling features (simplified - ideally query from DynamoDB)
    features.append(metric.get('latency_p99_5m_avg', metric.get('latency_p99', 0)))
    # ... add all FEATURE_COLUMNS in exact order

    return features

def generate_insight_sagemaker(metric: dict) -> dict:
    """Generate insight using SageMaker endpoint."""

    features = prepare_features(metric)

    # Invoke SageMaker endpoint
    response = sagemaker_runtime.invoke_endpoint(
        EndpointName=SAGEMAKER_ENDPOINT,
        ContentType='text/csv',
        Body=','.join(map(str, features)),
    )

    result = json.loads(response['Body'].read())

    # Parse SageMaker output
    prediction_type = result['prediction_type']
    risk_score = int(result['risk_score'])
    confidence = result.get('confidence', 0.9)

    # Generate explanation (rule-based or secondary model)
    explanation = generate_explanation(metric, prediction_type, risk_score)

    # Generate recommendations
    recommendations = generate_recommendations(prediction_type)

    return {
        'prediction_type': prediction_type,
        'risk_score': risk_score,
        'explanation': explanation,
        'recommendations': recommendations,
        'confidence': confidence,
        'model_used': 'xgboost-custom',
        'model_version': '1.3',
    }

def handler(event, context):
    """
    Enhanced AI Lambda with SageMaker + Bedrock fallback.
    """
    metrics = event.get('metrics', [event])
    insights = []

    for metric in metrics:
        try:
            if USE_SAGEMAKER:
                # Try SageMaker first
                insight = generate_insight_sagemaker(metric)
            else:
                # Use Bedrock
                insight = generate_insight_bedrock(metric)

        except Exception as e:
            print(f"SageMaker inference failed: {e}, falling back to Bedrock")
            insight = generate_insight_bedrock(metric)

        # Write to DynamoDB (same as before)
        item = {
            'entity_id': f"alert_{metric['stream_id']}_{int(datetime.now().timestamp())}",
            'timestamp': datetime.utcnow().isoformat() + 'Z',
            'entity_type': 'insight',
            **insight,
            'related_entity': metric['stream_id'],
            'ttl': int(datetime.now().timestamp()) + (30 * 24 * 60 * 60),
        }

        table.put_item(Item=item)
        insights.append(item)

        # Trigger EventBridge for critical
        if insight['risk_score'] >= 80:
            trigger_alert(item)

    return {'insights': insights, 'count': len(insights)}
```

---

## 4. Swarm Execution Commands

### Run Complete ML Training Pipeline with Swarm

```bash
# Initialize swarm for ML training
npx claude-flow swarm init --topology hierarchical --max-agents 8

# Spawn specialist agents
npx claude-flow agent spawn data-collector --capabilities "dynamodb,s3,parquet"
npx claude-flow agent spawn labeling-agent --capabilities "rule-based,expert-ui,validation"
npx claude-flow agent spawn feature-engineer --capabilities "pandas,feature-eng,normalization"
npx claude-flow agent spawn training-agent --capabilities "sagemaker,xgboost,hyperparameter-tuning"
npx claude-flow agent spawn evaluation-agent --capabilities "metrics,confusion-matrix,reporting"
npx claude-flow agent spawn deployment-agent --capabilities "sagemaker-endpoint,lambda-update,canary"

# Orchestrate entire training pipeline
npx claude-flow task orchestrate \
  --task "Execute complete ML training pipeline: data collection (30 days) â†’ auto-labeling (80% coverage) â†’ feature engineering (25 features) â†’ SageMaker training (XGBoost) â†’ evaluation (>90% accuracy target) â†’ deployment (canary)" \
  --strategy adaptive \
  --priority critical

# Monitor swarm progress
npx claude-flow swarm monitor --interval 10

# Check agent metrics
npx claude-flow agent metrics --agent-id training-agent

# Retrieve task results
npx claude-flow task results --task-id <task-id>
```

### Memory Coordination Commands

```bash
# Store training dataset location
npx claude-flow memory store \
  --key "ml-training/dataset-v1" \
  --value '{"s3_path": "s3://iops-ml-training/datasets/v1/", "samples": 100000, "features": 25}' \
  --namespace training

# Store model performance
npx claude-flow memory store \
  --key "ml-training/model-v1.3-metrics" \
  --value '{"accuracy": 0.942, "precision": 0.918, "recall": 0.935, "f1": 0.926}' \
  --namespace models

# Retrieve training status
npx claude-flow memory retrieve --key "ml-training/status" --namespace training
```

---

## 5. Cost Analysis

### Training Costs (One-Time)

| Component | Cost |
|-----------|------|
| Data Collection Lambda | $0.50 |
| S3 Storage (100K samples, 1GB) | $0.023/month |
| Expert Review Time (20 hours @ $100/hr) | $2,000 |
| SageMaker Training (ml.m5.xlarge, 2 hours) | $0.46 |
| Hyperparameter Tuning (50 jobs) | $23.00 |
| **Total One-Time Training Cost** | **~$2,024** |

### Inference Costs (Ongoing)

**Bedrock (Current PR-12):**
- Cost per insight: $0.000375
- 100K insights/month: $37.50/month

**SageMaker Endpoint (PR-13):**
- Endpoint (ml.t3.medium, 24/7): $31.39/month
- Inference cost: $0.00004 per request
- 100K inferences/month: $4.00/month
- **Total: $35.39/month**

**Cost Comparison:**
- **Bedrock**: $37.50/month
- **SageMaker**: $35.39/month
- **Savings**: $2.11/month (6% reduction)

**BUT**: At higher volumes, SageMaker wins dramatically:
- 1M insights/month:
  - Bedrock: $375/month
  - SageMaker: $35.39 + $40 = $75.39/month
  - **Savings: $299.61/month (80% reduction)**

---

## 6. Success Metrics

**Training Phase:**
- âœ… Collect 100K+ labeled samples
- âœ… Auto-labeling coverage >80%
- âœ… Expert review <20 hours
- âœ… Model accuracy >90%
- âœ… Training time <48 hours

**Deployment Phase:**
- âœ… Endpoint latency <50ms (P95)
- âœ… Model accuracy in production >88%
- âœ… Fallback rate <5%
- âœ… Zero deployment downtime

**Business Metrics:**
- âœ… Cost reduction >50% (at scale)
- âœ… Inference latency 10x faster
- âœ… Model retraining monthly
- âœ… Continuous improvement loop

---

## Timeline: 6-8 Weeks

**Week 1-2**: Data collection & labeling (100K samples)
**Week 3-4**: Feature engineering & model training
**Week 5**: Model evaluation & tuning
**Week 6**: Deployment & canary testing
**Week 7-8**: Monitoring & optimization

---

**Document Version:** 1.0
**Created:** November 5, 2025
**Dependencies:** PR-12 (AI Integration)
**Estimated Cost:** $2,024 one-time + $35/month
**Estimated Savings:** 80% at 1M+ insights/month
