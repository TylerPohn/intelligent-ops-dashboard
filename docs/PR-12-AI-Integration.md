# PR-12: Enable Real AI Inference with AWS Bedrock

## Overview

**Status:** Not Started
**Priority:** High
**Complexity:** High
**Estimated Effort:** 3-5 days

**Current State:** Test script generates simulated AI-like insights without calling Bedrock. AI Lambda exists but is not integrated into the active data flow.

**Goal:** Enable real-time AI inference using AWS Bedrock (Claude 3.5 Haiku) to analyze InfiniBand metrics and generate predictive insights.

---

## Current vs Target Architecture

### Current Architecture (Simulated AI)
```
┌─────────────┐      ┌──────────┐      ┌────────┐      ┌─────────────┐      ┌───────────┐
│ Test Script │─────►│ DynamoDB │─────►│ Lambda │─────►│ API Gateway │─────►│ Dashboard │
│  (Generate) │      │  (Store) │      │ (Query)│      │   (Serve)   │      │ (Display) │
└─────────────┘      └──────────┘      └────────┘      └─────────────┘      └───────────┘

✓ Active - No AI inference
✓ Insights pre-generated by script
✓ Fast and cheap for demos
✗ No real-time analysis
✗ No actual pattern detection
```

### Target Architecture (Real AI with Bedrock - Direct DynamoDB)
```
                        ┌─────────────────────────────────────────────────────┐
                        │        AWS Bedrock Integration Pipeline            │
                        │          (No Kinesis - Direct Writes)               │
                        └─────────────────────────────────────────────────────┘

┌────────────────┐      ┌─────────────────┐      ┌──────────────────────────┐
│ InfiniBand HPC │      │  API Gateway    │      │   Ingest Lambda          │
│   Monitoring   │─────►│  REST Endpoint  │─────►│  ┌────────────────────┐  │
│   (50+ Nodes)  │      │  /metrics       │      │  │ Validate Metrics   │  │
└────────────────┘      └─────────────────┘      │  │ Detect Patterns    │  │
                                                  │  │ Calculate Baselines│  │
                                                  │  │ Trigger AI if needed│ │
                                                  │  └────────────────────┘  │
                                                  └───────────┬──────────────┘
                                                              │
                        ┌─────────────────────────────────────┘
                        │
                        ▼
         ┌──────────────────────────────────────────────────────────────┐
         │                     DynamoDB Table                            │
         │              iops-dashboard-metrics                           │
         │  ┌──────────────────┐  ┌──────────────────┐                 │
         │  │  Raw Metrics     │  │   AI Insights    │                 │
         │  │  - Latency       │  │  - Predictions   │                 │
         │  │  - Utilization   │  │  - Risk Scores   │                 │
         │  │  - Error Rates   │  │  - Recommendations│                │
         │  └──────────────────┘  └──────────────────┘                 │
         └────────────┬──────────────────┬──────────────────────────────┘
                      │                  │
         ┌────────────┘                  └─────────────┐
         │                                              │
         ▼                                              ▼
┌─────────────────┐                          ┌──────────────────────┐
│  Query by GSI   │                          │    AI Lambda         │
│ EntityTypeIndex │                          │  ┌────────────────┐  │
│  (entity_type   │                          │  │ Bedrock Client │  │
│   = 'insight')  │                          │  │   Claude 3.5   │  │
└────────┬────────┘                          │  │     Haiku      │  │
         │                                   │  └────────┬───────┘  │
         │                                   │           │          │
         │                                   │  ┌────────▼───────┐  │
         │                                   │  │ Prompt Builder │  │
         │                                   │  │ - InfiniBand   │  │
         │                                   │  │   Context      │  │
         │                                   │  │ - Baselines    │  │
         │                                   │  │ - Thresholds   │  │
         │                                   │  └────────┬───────┘  │
         │                                   │           │          │
         │                                   │  ┌────────▼───────┐  │
         │                                   │  │ Parse Response │  │
         │                                   │  │ Write Insights │  │
         │                                   │  └────────┬───────┘  │
         │                                   └───────────┼──────────┘
         │                                               │
         │                                               │ (risk_score >= 80)
         │                                               ▼
         │                                   ┌────────────────────┐
         │                                   │   EventBridge      │
         │                                   │   Critical Alert   │
         │                                   │       Rule         │
         │                                   └─────────┬──────────┘
         │                                             │
         │                                             ▼
         │                                   ┌────────────────────┐
         │                                   │    SNS Topic       │
         │                                   │  Critical Alerts   │
         │                                   │  ┌──────────────┐  │
         │                                   │  │ Email Notify │  │
         │                                   │  └──────────────┘  │
         │                                   └────────────────────┘
         │
         ▼
┌─────────────────┐      ┌─────────────┐      ┌───────────┐
│   API Gateway   │─────►│  Dashboard  │      │  Polling  │
│ REST Endpoints  │      │   React UI  │◄─────│ Every 5s  │
│ /insights/recent│      │ TanStack Q. │      │           │
└─────────────────┘      └─────────────┘      └───────────┘

Legend:
━━━ Data Flow
│   Processing Step
┌─┐ AWS Service/Component
```

### Detailed Component Flow
```
Step 1: Event Ingestion (Direct Write - No Kinesis)
────────────────────────────────────────────────────
  InfiniBand Nodes → API Gateway → Ingest Lambda → DynamoDB
  (Raw telemetry)    (REST POST)   (Validation)    (Write metrics)

Step 2: Pattern Detection & AI Trigger
───────────────────────────────────────
  Ingest Lambda → Detect Patterns → AI Lambda (async invoke)
  (Baseline check)  (if anomaly)     (Bedrock inference)

Step 3: AI Inference Pipeline
──────────────────────────────
  AI Lambda → Build Prompt → Bedrock API → Parse Response
           (Context + Data)  (Claude 3.5)  (Extract JSON)
                                │
                                ├─► prediction_type
                                ├─► risk_score (0-100)
                                ├─► explanation
                                ├─► recommendations[]
                                └─► confidence (0.0-1.0)

Step 4: Insight Persistence
────────────────────────────
  AI Lambda → DynamoDB (insights table)
           → EntityTypeIndex GSI (for queries)

Step 5: Critical Alert Path
────────────────────────────
  AI Lambda → EventBridge → SNS → Email
  (risk >= 80) (Rule match)  (Topic) (Notify ops)

Step 6: Dashboard Display
──────────────────────────
  Dashboard → API Gateway → Lambda → DynamoDB Query
  (Poll 5s)   (REST)        (Get)    (EntityTypeIndex)
                                   → Return insights[]
```

---

## Work Breakdown

## 1. Architecture Changes

### 1.1 Connect Kinesis Event Source to Process Lambda
**File:** `cdk/lib/cdk-stack.ts`

**Changes:**
- Add EventSourceMapping from Kinesis stream to Process Lambda
- Configure batch size (100-500 records)
- Set starting position (LATEST)
- Add error handling and DLQ

**Code:**
```typescript
const processLambda = new lambda.Function(this, 'ProcessFunction', {
  runtime: lambda.Runtime.PYTHON_3_12,
  handler: 'handler.handler',
  code: lambda.Code.fromAsset('lambda/process'),
  timeout: Duration.seconds(60),
  environment: {
    METRICS_TABLE: metricsTable.tableName,
  },
});

// Add Kinesis event source
processLambda.addEventSource(
  new eventsources.KinesisEventSource(dataStream, {
    batchSize: 100,
    startingPosition: lambda.StartingPosition.LATEST,
    retryAttempts: 3,
    bisectBatchOnError: true,
    onFailure: new destinations.SqsDestination(dlq),
  })
);

metricsTable.grantWriteData(processLambda);
```

**Considerations:**
- Batch size affects latency vs throughput
- Starting position: LATEST (new events) vs TRIM_HORIZON (all events)
- Error handling: DLQ for failed batches

### 1.2 Set Up Process Lambda → AI Lambda Trigger
**File:** `lambda/process/handler.py`

**Changes:**
- Add logic to detect patterns requiring AI analysis
- Invoke AI Lambda asynchronously for qualifying metrics
- Implement throttling to control Bedrock costs

**Trigger Criteria:**
```python
def should_trigger_ai_analysis(metric: dict) -> bool:
    """
    Determine if metric requires AI inference.

    Criteria:
    - High risk metrics (utilization > 80%)
    - Sudden changes (>20% delta from baseline)
    - Error rate increases (>5% QP errors)
    - Latency spikes (>2x P99 baseline)
    """
    return (
        metric.get('utilization', 0) > 80 or
        metric.get('latency_p99', 0) > baseline * 2 or
        metric.get('error_rate', 0) > 0.05 or
        abs(metric.get('delta_pct', 0)) > 20
    )
```

**Code:**
```python
import boto3
import json
from typing import List, Dict, Any

lambda_client = boto3.client('lambda')
AI_LAMBDA_ARN = os.environ['AI_LAMBDA_ARN']

def process_kinesis_records(event: dict) -> None:
    """Process Kinesis batch and trigger AI analysis."""
    metrics = []

    for record in event['Records']:
        payload = json.loads(base64.b64decode(record['kinesis']['data']))

        # Aggregate and enrich metric
        metric = aggregate_metric(payload)

        # Store in DynamoDB
        store_metric(metric)

        # Check if AI analysis needed
        if should_trigger_ai_analysis(metric):
            metrics.append(metric)

    # Batch invoke AI Lambda (avoid per-metric invocations)
    if metrics:
        lambda_client.invoke(
            FunctionName=AI_LAMBDA_ARN,
            InvocationType='Event',  # Asynchronous
            Payload=json.dumps({'metrics': metrics}),
        )
```

### 1.3 Configure AI Lambda → DynamoDB Writes
**File:** `lambda/ai/handler.py`

**Changes:**
- Update to write insights directly to DynamoDB
- Use same schema as test script for compatibility
- Add batch writing for multiple insights

**Current State:**
```python
def handler(event, context):
    """Current AI Lambda - only logs output"""
    prompt = f"""Analyze InfiniBand metric: {json.dumps(event, indent=2)}
    Provide: prediction_type, risk_score, explanation, recommendations"""

    response = bedrock_client.invoke_model(
        modelId='anthropic.claude-3-5-haiku-20241022-v1:0',
        body=json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 1000,
            "messages": [{"role": "user", "content": prompt}],
        }),
    )

    # Currently just returns result, doesn't store
    result = json.loads(response['body'].read())
    print(f"AI Inference: {result}")
    return result
```

**Target State:**
```python
import boto3
from datetime import datetime
from typing import List, Dict, Any

dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table(os.environ['METRICS_TABLE'])

def handler(event, context):
    """Enhanced AI Lambda with DynamoDB writes."""
    metrics = event.get('metrics', [event])  # Support batch or single
    insights = []

    for metric in metrics:
        # Generate AI insight
        insight = generate_insight(metric)

        # Write to DynamoDB
        item = {
            'entity_id': f"alert_{metric['stream_id']}_{int(datetime.now().timestamp())}",
            'timestamp': datetime.utcnow().isoformat() + 'Z',
            'entity_type': 'insight',
            'prediction_type': insight['prediction_type'],
            'risk_score': insight['risk_score'],
            'explanation': insight['explanation'],
            'recommendations': insight['recommendations'],
            'model_used': 'claude-3-5-haiku',
            'confidence': insight['confidence'],
            'related_entity': metric['stream_id'],
            'ttl': int(datetime.now().timestamp()) + (30 * 24 * 60 * 60),  # 30 days
        }

        table.put_item(Item=item)
        insights.append(item)

        # Trigger EventBridge for critical insights
        if insight['risk_score'] >= 80:
            trigger_alert(item)

    return {'insights': insights, 'count': len(insights)}

def generate_insight(metric: dict) -> dict:
    """Call Bedrock for AI analysis."""
    prompt = build_infiniband_prompt(metric)

    response = bedrock_client.invoke_model(
        modelId='anthropic.claude-3-5-haiku-20241022-v1:0',
        body=json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 1000,
            "temperature": 0.3,  # More deterministic for production
            "messages": [{"role": "user", "content": prompt}],
        }),
    )

    result = json.loads(response['body'].read())
    return parse_bedrock_response(result)
```

### 1.4 Wire Up EventBridge Alerts
**File:** `cdk/lib/cdk-stack.ts`

**Changes:**
- Create EventBridge rule for critical insights (risk_score >= 80)
- Connect to SNS topic for email alerts
- Add CloudWatch Log Group target for audit trail

**Code:**
```typescript
// SNS topic for critical alerts
const alertTopic = new sns.Topic(this, 'CriticalAlertTopic', {
  displayName: 'IOPS Critical Alerts',
});

// Subscribe email addresses
alertTopic.addSubscription(
  new subscriptions.EmailSubscription(process.env.CRITICAL_ALERT_EMAILS || 'alerts@example.com')
);

// EventBridge rule triggered by AI Lambda
const criticalAlertRule = new events.Rule(this, 'CriticalAlertRule', {
  eventPattern: {
    source: ['iops.ai.lambda'],
    detailType: ['Critical Insight'],
    detail: {
      risk_score: [{ numeric: ['>=', 80] }],
    },
  },
});

criticalAlertRule.addTarget(new targets.SnsTopic(alertTopic));
criticalAlertRule.addTarget(new targets.CloudWatchLogGroup(alertLogGroup));
```

**AI Lambda Changes:**
```python
import boto3

eventbridge = boto3.client('events')

def trigger_alert(insight: dict) -> None:
    """Send critical insight to EventBridge."""
    eventbridge.put_events(
        Entries=[{
            'Source': 'iops.ai.lambda',
            'DetailType': 'Critical Insight',
            'Detail': json.dumps({
                'alert_id': insight['entity_id'],
                'risk_score': insight['risk_score'],
                'prediction_type': insight['prediction_type'],
                'explanation': insight['explanation'],
                'stream_id': insight['related_entity'],
                'timestamp': insight['timestamp'],
            }),
        }]
    )
```

---

## 2. Code Changes

### 2.1 Process Lambda: Pattern Detection
**File:** `lambda/process/handler.py`

**New Functions:**
```python
def calculate_baseline(stream_id: str, metric_type: str, lookback_minutes: int = 15) -> float:
    """Calculate baseline from historical data."""
    response = table.query(
        KeyConditionExpression=Key('entity_id').eq(stream_id) & Key('timestamp').between(
            (datetime.now() - timedelta(minutes=lookback_minutes)).isoformat(),
            datetime.now().isoformat()
        ),
        IndexName='EntityTypeIndex',
    )

    values = [item.get(metric_type, 0) for item in response['Items']]
    return statistics.mean(values) if values else 0

def detect_anomaly(metric: dict, baseline: float) -> bool:
    """Statistical anomaly detection."""
    value = metric.get('current_value', 0)
    threshold = baseline * 1.5  # 50% above baseline
    return value > threshold

def aggregate_metric(payload: dict) -> dict:
    """
    Aggregate raw Kinesis event into enriched metric.

    Input: Raw InfiniBand event
    Output: Enriched metric with context for AI
    """
    stream_id = payload['stream_id']

    # Calculate baseline
    baseline_latency = calculate_baseline(stream_id, 'latency_p99')
    baseline_utilization = calculate_baseline(stream_id, 'utilization')

    # Enrich metric
    metric = {
        'stream_id': stream_id,
        'timestamp': payload['timestamp'],
        'latency_p99': payload.get('latency_p99', 0),
        'latency_delta': payload.get('latency_p99', 0) - baseline_latency,
        'utilization': payload.get('utilization', 0),
        'utilization_delta': payload.get('utilization', 0) - baseline_utilization,
        'error_rate': payload.get('error_rate', 0),
        'bandwidth_gbps': payload.get('bandwidth_gbps', 0),
        'qp_errors': payload.get('qp_errors', 0),
        'baseline_latency': baseline_latency,
        'baseline_utilization': baseline_utilization,
        'is_anomaly': detect_anomaly(payload, baseline_latency),
    }

    return metric
```

### 2.2 AI Lambda: Prompt Engineering
**File:** `lambda/ai/handler.py`

**Enhanced Prompts:**
```python
def build_infiniband_prompt(metric: dict) -> str:
    """
    Build context-aware prompt for InfiniBand analysis.

    Key improvements:
    - Specific InfiniBand terminology
    - Baseline comparisons
    - Historical context
    - Actionable output format
    """
    return f"""You are an InfiniBand network operations expert analyzing real-time metrics.

METRIC DATA:
- Stream ID: {metric['stream_id']}
- Timestamp: {metric['timestamp']}
- Current P99 Latency: {metric['latency_p99']}μs (baseline: {metric['baseline_latency']}μs)
- Port Utilization: {metric['utilization']}% (baseline: {metric['baseline_utilization']}%)
- Error Rate: {metric['error_rate']}%
- QP Errors: {metric['qp_errors']}
- Bandwidth: {metric['bandwidth_gbps']} Gbps
- Anomaly Detected: {metric['is_anomaly']}

CONTEXT:
- This is an active production InfiniBand fabric supporting HPC workloads
- Normal P99 latency: <5μs
- Normal utilization: 40-70%
- Acceptable error rate: <0.1%

ANALYSIS REQUIRED:
Analyze the above metric and provide:

1. **prediction_type**: One of [performance_degradation, anomaly_detected, resource_saturation,
   connection_instability, packet_loss_detected, latency_spike, bandwidth_throttling,
   qp_error_rate_high, memory_registration_failure, rdma_timeout]

2. **risk_score**: Integer 0-100 based on severity
   - 80-100: Critical, immediate action required
   - 60-79: High, investigate within 1 hour
   - 40-59: Medium, monitor closely
   - 0-39: Low, informational

3. **explanation**: Technical explanation referencing specific metrics and deltas from baseline

4. **recommendations**: Array of 2-4 specific, actionable remediation steps

5. **confidence**: Float 0.0-1.0 representing prediction confidence

OUTPUT FORMAT (JSON only):
{{
  "prediction_type": "...",
  "risk_score": 85,
  "explanation": "...",
  "recommendations": ["...", "..."],
  "confidence": 0.92
}}"""

def parse_bedrock_response(response: dict) -> dict:
    """Parse and validate Bedrock response."""
    content = response['content'][0]['text']

    # Extract JSON from response (handle markdown code blocks)
    json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
    if json_match:
        content = json_match.group(1)

    try:
        result = json.loads(content)
    except json.JSONDecodeError:
        # Fallback parsing
        result = {
            'prediction_type': 'unknown',
            'risk_score': 50,
            'explanation': content[:500],
            'recommendations': ['Review metric manually'],
            'confidence': 0.5,
        }

    # Validate schema
    assert 'prediction_type' in result
    assert 'risk_score' in result
    assert 0 <= result['risk_score'] <= 100
    assert 'recommendations' in result and isinstance(result['recommendations'], list)

    return result
```

### 2.3 Error Handling and Retry Logic
**File:** `lambda/ai/handler.py`

**Robust Error Handling:**
```python
from botocore.exceptions import ClientError
import time

MAX_RETRIES = 3
RETRY_DELAY = 1  # seconds

def generate_insight_with_retry(metric: dict, retries: int = MAX_RETRIES) -> dict:
    """Call Bedrock with exponential backoff retry."""
    for attempt in range(retries):
        try:
            return generate_insight(metric)

        except ClientError as e:
            error_code = e.response['Error']['Code']

            # Throttling - retry with backoff
            if error_code == 'ThrottlingException':
                if attempt < retries - 1:
                    delay = RETRY_DELAY * (2 ** attempt)
                    print(f"Throttled, retrying in {delay}s...")
                    time.sleep(delay)
                    continue
                else:
                    print(f"Max retries exceeded for throttling")
                    return fallback_insight(metric)

            # Service errors - retry
            elif error_code in ['ServiceUnavailable', 'InternalServerError']:
                if attempt < retries - 1:
                    time.sleep(RETRY_DELAY)
                    continue
                else:
                    return fallback_insight(metric)

            # Client errors - don't retry
            elif error_code in ['ValidationException', 'AccessDeniedException']:
                print(f"Client error: {error_code}")
                return fallback_insight(metric)

            else:
                raise

        except Exception as e:
            print(f"Unexpected error: {str(e)}")
            return fallback_insight(metric)

    return fallback_insight(metric)

def fallback_insight(metric: dict) -> dict:
    """Generate rule-based insight when AI fails."""
    if metric['utilization'] > 90:
        return {
            'prediction_type': 'resource_saturation',
            'risk_score': 85,
            'explanation': f"Port utilization at {metric['utilization']}% (fallback analysis)",
            'recommendations': ['Scale workload', 'Check for hot spots'],
            'confidence': 0.6,
        }
    elif metric['latency_delta'] > metric['baseline_latency'] * 0.5:
        return {
            'prediction_type': 'performance_degradation',
            'risk_score': 70,
            'explanation': f"Latency increased by {metric['latency_delta']}μs (fallback analysis)",
            'recommendations': ['Check network topology', 'Review switch configs'],
            'confidence': 0.6,
        }
    else:
        return {
            'prediction_type': 'anomaly_detected',
            'risk_score': 50,
            'explanation': 'Anomaly detected, AI analysis unavailable (fallback)',
            'recommendations': ['Manual investigation required'],
            'confidence': 0.5,
        }
```

---

## 3. Infrastructure Requirements

### 3.1 IAM Permissions

**Process Lambda Role:**
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "dynamodb:PutItem",
        "dynamodb:Query"
      ],
      "Resource": [
        "arn:aws:dynamodb:us-east-2:*:table/iops-dashboard-metrics",
        "arn:aws:dynamodb:us-east-2:*:table/iops-dashboard-metrics/index/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "lambda:InvokeFunction"
      ],
      "Resource": "arn:aws:lambda:us-east-2:*:function:IOpsDashboard-AIFunction*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "kinesis:GetRecords",
        "kinesis:GetShardIterator",
        "kinesis:DescribeStream",
        "kinesis:ListShards"
      ],
      "Resource": "arn:aws:kinesis:us-east-2:*:stream/iops-dashboard-events"
    }
  ]
}
```

**AI Lambda Role:**
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "bedrock:InvokeModel"
      ],
      "Resource": "arn:aws:bedrock:us-east-2::foundation-model/anthropic.claude-3-5-haiku-20241022-v1:0"
    },
    {
      "Effect": "Allow",
      "Action": [
        "dynamodb:PutItem",
        "dynamodb:BatchWriteItem"
      ],
      "Resource": "arn:aws:dynamodb:us-east-2:*:table/iops-dashboard-metrics"
    },
    {
      "Effect": "Allow",
      "Action": [
        "events:PutEvents"
      ],
      "Resource": "arn:aws:events:us-east-2:*:event-bus/default"
    }
  ]
}
```

### 3.2 EventBridge Configuration

**Rule Definition:**
```json
{
  "Name": "IOpsDashboard-CriticalInsights",
  "State": "ENABLED",
  "EventPattern": {
    "source": ["iops.ai.lambda"],
    "detail-type": ["Critical Insight"],
    "detail": {
      "risk_score": [{ "numeric": [">=", 80] }]
    }
  },
  "Targets": [
    {
      "Arn": "arn:aws:sns:us-east-2:*:IOpsDashboard-CriticalAlerts",
      "Id": "SNSTarget"
    },
    {
      "Arn": "arn:aws:logs:us-east-2:*:log-group:/aws/events/iops-critical-alerts",
      "Id": "CloudWatchLogsTarget"
    }
  ]
}
```

### 3.3 SNS Topic Configuration

**Topic Policy:**
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "events.amazonaws.com"
      },
      "Action": "SNS:Publish",
      "Resource": "arn:aws:sns:us-east-2:*:IOpsDashboard-CriticalAlerts"
    }
  ]
}
```

**Email Template:**
```
Subject: [CRITICAL] IOPS Dashboard Alert - {{detail.prediction_type}}

Alert ID: {{detail.alert_id}}
Stream: {{detail.stream_id}}
Risk Score: {{detail.risk_score}}/100
Timestamp: {{detail.timestamp}}

Issue Type: {{detail.prediction_type}}

Explanation:
{{detail.explanation}}

Dashboard: https://your-dashboard-url.com

This is an automated alert from the IOPS Dashboard AI monitoring system.
```

### 3.4 CloudWatch Alarms

**AI Lambda Errors:**
```typescript
const aiErrorAlarm = new cloudwatch.Alarm(this, 'AILambdaErrorAlarm', {
  metric: aiLambda.metricErrors(),
  threshold: 5,
  evaluationPeriods: 1,
  datapointsToAlarm: 1,
  treatMissingData: cloudwatch.TreatMissingData.NOT_BREACHING,
});

aiErrorAlarm.addAlarmAction(new actions.SnsAction(alertTopic));
```

**Bedrock Throttling:**
```typescript
const bedrockThrottleAlarm = new cloudwatch.Alarm(this, 'BedrockThrottleAlarm', {
  metric: new cloudwatch.Metric({
    namespace: 'AWS/Bedrock',
    metricName: 'ModelInvocationThrottles',
    dimensionsMap: {
      ModelId: 'anthropic.claude-3-5-haiku-20241022-v1:0',
    },
    statistic: 'Sum',
  }),
  threshold: 10,
  evaluationPeriods: 5,
  datapointsToAlarm: 3,
});

bedrockThrottleAlarm.addAlarmAction(new actions.SnsAction(alertTopic));
```

---

## 4. Bedrock Configuration

### 4.1 Model Selection

**Chosen Model:** `anthropic.claude-3-5-haiku-20241022-v1:0`

**Rationale:**
- **Fast:** 5-10ms inference latency (suitable for real-time)
- **Cost-effective:** ~$0.25 per 1M input tokens
- **Capable:** Strong reasoning for pattern detection
- **JSON support:** Native structured output

**Alternative Models:**
- **Claude 3.5 Sonnet:** More capable but 3x slower, 4x cost
- **Claude 3 Opus:** Highest capability but 10x cost
- **Use case:** Haiku optimal for high-volume real-time inference

### 4.2 Temperature and Token Settings

```python
BEDROCK_CONFIG = {
    'modelId': 'anthropic.claude-3-5-haiku-20241022-v1:0',
    'temperature': 0.3,  # Low for deterministic production output
    'max_tokens': 1000,  # Sufficient for structured output
    'top_p': 0.9,
    'anthropic_version': 'bedrock-2023-05-31',
}
```

**Parameter Rationale:**
- **Temperature 0.3:** More deterministic, consistent predictions
- **Max tokens 1000:** Covers explanation + recommendations
- **Top_p 0.9:** Balance between diversity and coherence

### 4.3 Prompt Engineering Best Practices

**Key Principles:**
1. **Context First:** Provide baseline and historical data
2. **Specific Format:** Request JSON with exact schema
3. **Domain Terms:** Use InfiniBand terminology (QP, HCA, RDMA)
4. **Clear Constraints:** Define risk score ranges explicitly
5. **Examples:** Include few-shot examples for edge cases

**Few-Shot Prompt Example:**
```python
EXAMPLE_PROMPT = """
EXAMPLE 1:
Input: latency_p99=12μs (baseline=4μs), utilization=45%
Output: {
  "prediction_type": "latency_spike",
  "risk_score": 65,
  "explanation": "P99 latency increased 3x above baseline...",
  "recommendations": ["Check for network congestion", "Review QoS settings"],
  "confidence": 0.88
}

EXAMPLE 2:
Input: utilization=95%, error_rate=0.02%
Output: {
  "prediction_type": "resource_saturation",
  "risk_score": 90,
  "explanation": "Port utilization critical at 95%...",
  "recommendations": ["Scale workload immediately", "Add redundant paths"],
  "confidence": 0.95
}

NOW ANALYZE THE FOLLOWING METRIC:
{metric_data}
"""
```

### 4.4 Rate Limiting and Throttling

**Bedrock Quotas (us-east-2):**
- **Haiku:** 2,000 requests per minute (RPM)
- **Burst:** 20,000 tokens per minute (TPM)
- **Soft limit:** Request increase via AWS Support

**Implementation:**
```python
import time
from collections import deque

class BedrockRateLimiter:
    """Token bucket rate limiter for Bedrock API."""

    def __init__(self, max_rpm: int = 1800):  # 90% of quota
        self.max_rpm = max_rpm
        self.requests = deque()

    def acquire(self) -> None:
        """Wait if rate limit exceeded."""
        now = time.time()

        # Remove requests older than 1 minute
        while self.requests and self.requests[0] < now - 60:
            self.requests.popleft()

        # Check if at limit
        if len(self.requests) >= self.max_rpm:
            sleep_time = 60 - (now - self.requests[0])
            if sleep_time > 0:
                print(f"Rate limit reached, sleeping {sleep_time:.2f}s")
                time.sleep(sleep_time)
                self.requests.clear()

        self.requests.append(now)

# Global rate limiter
rate_limiter = BedrockRateLimiter(max_rpm=1800)

def generate_insight(metric: dict) -> dict:
    """Generate insight with rate limiting."""
    rate_limiter.acquire()

    response = bedrock_client.invoke_model(
        modelId='anthropic.claude-3-5-haiku-20241022-v1:0',
        body=json.dumps({...}),
    )

    return parse_bedrock_response(response)
```

---

## 5. Testing Strategy

### 5.1 Unit Tests

**File:** `lambda/ai/test_handler.py`

```python
import pytest
from unittest.mock import Mock, patch
from handler import (
    handler,
    generate_insight,
    parse_bedrock_response,
    fallback_insight,
    build_infiniband_prompt,
)

@pytest.fixture
def sample_metric():
    return {
        'stream_id': 'ib_stream_test',
        'timestamp': '2025-11-05T12:00:00Z',
        'latency_p99': 15,
        'baseline_latency': 5,
        'utilization': 85,
        'baseline_utilization': 60,
        'error_rate': 0.01,
        'bandwidth_gbps': 95,
        'qp_errors': 5,
        'is_anomaly': True,
    }

@pytest.fixture
def sample_bedrock_response():
    return {
        'content': [{
            'text': '''{
                "prediction_type": "performance_degradation",
                "risk_score": 75,
                "explanation": "Latency increased 3x above baseline",
                "recommendations": ["Check network", "Review configs"],
                "confidence": 0.92
            }'''
        }]
    }

class TestBedrockIntegration:
    """Test Bedrock API integration."""

    @patch('handler.bedrock_client')
    def test_generate_insight_success(self, mock_bedrock, sample_metric, sample_bedrock_response):
        """Test successful Bedrock invocation."""
        mock_bedrock.invoke_model.return_value = {
            'body': Mock(read=lambda: json.dumps(sample_bedrock_response).encode())
        }

        result = generate_insight(sample_metric)

        assert result['prediction_type'] == 'performance_degradation'
        assert result['risk_score'] == 75
        assert len(result['recommendations']) == 2
        assert 0 <= result['confidence'] <= 1

    @patch('handler.bedrock_client')
    def test_generate_insight_throttling(self, mock_bedrock, sample_metric):
        """Test throttling error handling."""
        mock_bedrock.invoke_model.side_effect = ClientError(
            {'Error': {'Code': 'ThrottlingException'}},
            'InvokeModel'
        )

        result = generate_insight_with_retry(sample_metric)

        # Should fallback after retries
        assert result['prediction_type'] in ['resource_saturation', 'performance_degradation', 'anomaly_detected']
        assert result['confidence'] == 0.6  # Fallback confidence

    def test_prompt_building(self, sample_metric):
        """Test prompt includes all necessary context."""
        prompt = build_infiniband_prompt(sample_metric)

        assert 'ib_stream_test' in prompt
        assert '15μs' in prompt  # Current latency
        assert '5μs' in prompt   # Baseline
        assert '85%' in prompt   # Utilization
        assert 'JSON' in prompt  # Format instruction

class TestResponseParsing:
    """Test Bedrock response parsing."""

    def test_parse_valid_json(self, sample_bedrock_response):
        """Test parsing valid JSON response."""
        result = parse_bedrock_response(sample_bedrock_response)

        assert 'prediction_type' in result
        assert 'risk_score' in result
        assert isinstance(result['recommendations'], list)

    def test_parse_markdown_json(self):
        """Test parsing JSON wrapped in markdown."""
        response = {
            'content': [{
                'text': '```json\n{"prediction_type": "anomaly_detected", "risk_score": 60}\n```'
            }]
        }

        result = parse_bedrock_response(response)
        assert result['prediction_type'] == 'anomaly_detected'

    def test_parse_invalid_json(self):
        """Test graceful handling of invalid JSON."""
        response = {
            'content': [{
                'text': 'This is not JSON at all'
            }]
        }

        result = parse_bedrock_response(response)

        # Should return fallback structure
        assert result['prediction_type'] == 'unknown'
        assert result['risk_score'] == 50
        assert isinstance(result['recommendations'], list)

class TestFallbackLogic:
    """Test fallback insights when AI fails."""

    def test_fallback_high_utilization(self):
        """Test fallback for resource saturation."""
        metric = {
            'utilization': 95,
            'baseline_utilization': 60,
            'latency_delta': 2,
            'baseline_latency': 5,
        }

        result = fallback_insight(metric)

        assert result['prediction_type'] == 'resource_saturation'
        assert result['risk_score'] >= 80
        assert result['confidence'] == 0.6

    def test_fallback_high_latency(self):
        """Test fallback for performance degradation."""
        metric = {
            'utilization': 60,
            'baseline_utilization': 60,
            'latency_delta': 10,
            'baseline_latency': 5,
        }

        result = fallback_insight(metric)

        assert result['prediction_type'] == 'performance_degradation'
        assert result['risk_score'] >= 60

class TestDynamoDBWrites:
    """Test DynamoDB insight storage."""

    @patch('handler.table')
    def test_insight_storage(self, mock_table, sample_metric):
        """Test insights written to DynamoDB."""
        event = {'metrics': [sample_metric]}

        with patch('handler.generate_insight') as mock_generate:
            mock_generate.return_value = {
                'prediction_type': 'anomaly_detected',
                'risk_score': 65,
                'explanation': 'Test explanation',
                'recommendations': ['Action 1', 'Action 2'],
                'confidence': 0.85,
            }

            handler(event, None)

            # Verify DynamoDB put_item called
            assert mock_table.put_item.called

            # Verify item structure
            call_args = mock_table.put_item.call_args[1]
            item = call_args['Item']

            assert item['entity_type'] == 'insight'
            assert item['risk_score'] == 65
            assert 'ttl' in item

class TestEventBridgeIntegration:
    """Test EventBridge alert triggering."""

    @patch('handler.eventbridge')
    def test_critical_alert_triggered(self, mock_eventbridge):
        """Test critical insights trigger EventBridge."""
        insight = {
            'entity_id': 'alert_test_123',
            'risk_score': 85,
            'prediction_type': 'resource_saturation',
            'explanation': 'Critical issue detected',
            'related_entity': 'ib_stream_1',
            'timestamp': '2025-11-05T12:00:00Z',
        }

        trigger_alert(insight)

        # Verify EventBridge put_events called
        assert mock_eventbridge.put_events.called

        # Verify event structure
        call_args = mock_eventbridge.put_events.call_args[1]
        entries = call_args['Entries']

        assert entries[0]['Source'] == 'iops.ai.lambda'
        assert entries[0]['DetailType'] == 'Critical Insight'
        assert 'alert_test_123' in entries[0]['Detail']

    @patch('handler.eventbridge')
    def test_low_risk_no_alert(self, mock_eventbridge):
        """Test low-risk insights don't trigger alerts."""
        insight = {
            'risk_score': 40,  # Below threshold
        }

        # Should not call trigger_alert for low risk
        # (This would be in main handler logic)
        if insight['risk_score'] >= 80:
            trigger_alert(insight)

        assert not mock_eventbridge.put_events.called
```

### 5.2 Integration Tests

**File:** `tests/integration/test_ai_pipeline.py`

```python
import boto3
import json
import time
from datetime import datetime

kinesis = boto3.client('kinesis', region_name='us-east-2')
dynamodb = boto3.resource('dynamodb', region_name='us-east-2')
table = dynamodb.Table('iops-dashboard-metrics')

class TestAIPipeline:
    """End-to-end integration tests for AI pipeline."""

    def test_kinesis_to_insight_flow(self):
        """Test complete flow: Kinesis → Process → AI → DynamoDB."""
        # 1. Put event to Kinesis
        test_event = {
            'stream_id': 'ib_stream_integration_test',
            'timestamp': datetime.utcnow().isoformat() + 'Z',
            'latency_p99': 25,
            'utilization': 90,
            'error_rate': 0.05,
            'bandwidth_gbps': 95,
            'qp_errors': 10,
        }

        response = kinesis.put_record(
            StreamName='iops-dashboard-events',
            Data=json.dumps(test_event),
            PartitionKey=test_event['stream_id'],
        )

        assert response['ResponseMetadata']['HTTPStatusCode'] == 200

        # 2. Wait for processing (Lambda trigger + AI inference)
        time.sleep(30)  # Allow time for async processing

        # 3. Query DynamoDB for insight
        result = table.query(
            IndexName='EntityTypeIndex',
            KeyConditionExpression='entity_type = :type',
            ExpressionAttributeValues={':type': 'insight'},
            ScanIndexForward=False,
            Limit=10,
        )

        # 4. Verify insight exists
        insights = [
            item for item in result['Items']
            if item.get('related_entity') == 'ib_stream_integration_test'
        ]

        assert len(insights) > 0, "No insight generated for test event"

        insight = insights[0]
        assert insight['prediction_type'] in [
            'performance_degradation', 'resource_saturation', 'anomaly_detected'
        ]
        assert 0 <= insight['risk_score'] <= 100
        assert insight['model_used'] == 'claude-3-5-haiku'
        assert len(insight['recommendations']) >= 2

    def test_high_volume_throughput(self):
        """Test pipeline handles high event volume."""
        # Send 100 events rapidly
        events = [
            {
                'stream_id': f'ib_stream_{i}',
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'latency_p99': 10 + (i % 20),
                'utilization': 60 + (i % 30),
            }
            for i in range(100)
        ]

        start_time = time.time()

        for event in events:
            kinesis.put_record(
                StreamName='iops-dashboard-events',
                Data=json.dumps(event),
                PartitionKey=event['stream_id'],
            )

        send_duration = time.time() - start_time
        print(f"Sent 100 events in {send_duration:.2f}s")

        # Wait for processing
        time.sleep(60)

        # Verify insights generated
        result = table.query(
            IndexName='EntityTypeIndex',
            KeyConditionExpression='entity_type = :type',
            ExpressionAttributeValues={':type': 'insight'},
            ScanIndexForward=False,
            Limit=100,
        )

        recent_insights = [
            item for item in result['Items']
            if item['timestamp'] > datetime.utcnow().isoformat()[:-10]  # Last minute
        ]

        # Should process at least 50% of events (some may not trigger AI)
        assert len(recent_insights) >= 50, f"Only {len(recent_insights)} insights generated"
```

### 5.3 Load Testing with Bedrock API

**File:** `tests/load/test_bedrock_load.py`

```python
import boto3
import time
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed

bedrock = boto3.client('bedrock-runtime', region_name='us-east-2')

def invoke_bedrock(metric: dict) -> dict:
    """Single Bedrock invocation."""
    start = time.time()

    try:
        response = bedrock.invoke_model(
            modelId='anthropic.claude-3-5-haiku-20241022-v1:0',
            body=json.dumps({
                'anthropic_version': 'bedrock-2023-05-31',
                'max_tokens': 1000,
                'temperature': 0.3,
                'messages': [{
                    'role': 'user',
                    'content': f'Analyze: {json.dumps(metric)}'
                }],
            }),
        )

        latency = time.time() - start
        return {'success': True, 'latency': latency}

    except Exception as e:
        latency = time.time() - start
        return {'success': False, 'latency': latency, 'error': str(e)}

class TestBedrockLoad:
    """Load test Bedrock API performance."""

    def test_sequential_throughput(self):
        """Test sequential request throughput."""
        metrics = [{'value': i} for i in range(50)]

        start_time = time.time()
        results = [invoke_bedrock(m) for m in metrics]
        duration = time.time() - start_time

        successes = [r for r in results if r['success']]
        latencies = [r['latency'] for r in results]

        print(f"\n=== Sequential Load Test ===")
        print(f"Total requests: {len(results)}")
        print(f"Successful: {len(successes)}")
        print(f"Duration: {duration:.2f}s")
        print(f"Throughput: {len(results)/duration:.2f} req/s")
        print(f"Mean latency: {statistics.mean(latencies):.3f}s")
        print(f"P95 latency: {statistics.quantiles(latencies, n=20)[18]:.3f}s")

        assert len(successes) >= 45, "Too many failures"
        assert statistics.mean(latencies) < 2.0, "Latency too high"

    def test_concurrent_throughput(self):
        """Test concurrent request handling."""
        metrics = [{'value': i} for i in range(100)]

        start_time = time.time()

        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = [executor.submit(invoke_bedrock, m) for m in metrics]
            results = [f.result() for f in as_completed(futures)]

        duration = time.time() - start_time

        successes = [r for r in results if r['success']]
        failures = [r for r in results if not r['success']]
        latencies = [r['latency'] for r in results]

        print(f"\n=== Concurrent Load Test ===")
        print(f"Total requests: {len(results)}")
        print(f"Successful: {len(successes)}")
        print(f"Failed: {len(failures)}")
        print(f"Duration: {duration:.2f}s")
        print(f"Throughput: {len(results)/duration:.2f} req/s")
        print(f"Mean latency: {statistics.mean(latencies):.3f}s")
        print(f"P95 latency: {statistics.quantiles(latencies, n=20)[18]:.3f}s")

        # Check for throttling
        throttle_errors = [
            r for r in failures
            if 'ThrottlingException' in r.get('error', '')
        ]

        if throttle_errors:
            print(f"Throttled requests: {len(throttle_errors)}")

        # Should handle at least 80% successfully at this volume
        assert len(successes) >= 80, f"Too many failures: {len(failures)}"
```

---

## 6. Migration Plan

### 6.1 Feature Flag Implementation

**File:** `lambda/process/handler.py`

```python
import os

AI_ENABLED = os.environ.get('AI_ENABLED', 'false').lower() == 'true'
AI_SAMPLING_RATE = float(os.environ.get('AI_SAMPLING_RATE', '1.0'))  # 0.0-1.0

def process_kinesis_records(event: dict) -> None:
    """Process records with AI feature flag."""
    metrics = []

    for record in event['Records']:
        payload = json.loads(base64.b64decode(record['kinesis']['data']))
        metric = aggregate_metric(payload)

        # Store metric regardless of AI
        store_metric(metric)

        # Conditional AI analysis
        if AI_ENABLED and should_trigger_ai_analysis(metric):
            # Sampling for gradual rollout
            if random.random() < AI_SAMPLING_RATE:
                metrics.append(metric)

    # Batch invoke AI Lambda
    if metrics:
        invoke_ai_lambda(metrics)
```

**CDK Environment Variables:**
```typescript
const processLambda = new lambda.Function(this, 'ProcessFunction', {
  environment: {
    AI_ENABLED: process.env.AI_ENABLED || 'false',
    AI_SAMPLING_RATE: process.env.AI_SAMPLING_RATE || '0.1',  // Start at 10%
  },
});
```

### 6.2 Gradual Rollout Strategy

**Phase 1: Development (1 week)**
- Deploy with `AI_ENABLED=false`
- Verify all infrastructure created correctly
- Test manually with feature flag enabled locally
- Monitor for deployment issues

**Phase 2: Canary (1 week)**
- Enable AI for 10% of traffic (`AI_SAMPLING_RATE=0.1`)
- Compare AI insights vs simulated insights
- Monitor Bedrock costs
- Check for errors and throttling
- Validate insight quality

**Phase 3: Ramp Up (2 weeks)**
- Increase sampling: 10% → 25% → 50% → 75%
- Monitor each stage for 2-3 days
- Compare cost projections vs actuals
- Gather user feedback on insight quality
- Tune prompts based on real data

**Phase 4: Full Rollout (1 week)**
- Set `AI_SAMPLING_RATE=1.0` (100% traffic)
- Monitor for 48 hours
- Verify all insights AI-generated
- Disable test script simulation
- Update documentation

**Phase 5: Optimization (ongoing)**
- Fine-tune prompts based on false positives/negatives
- Adjust risk score thresholds
- Optimize Bedrock parameters
- Implement cost controls

### 6.3 Fallback to Simulated Insights

**Hybrid Mode Implementation:**
```python
def generate_insight_hybrid(metric: dict) -> dict:
    """
    Generate insight with fallback.

    Strategy:
    1. Try AI inference
    2. If fails, use rule-based fallback
    3. Flag source in insight
    """
    try:
        if AI_ENABLED:
            insight = generate_insight_with_retry(metric)
            insight['source'] = 'ai'
            return insight
    except Exception as e:
        print(f"AI inference failed: {e}, using fallback")

    # Fallback to rule-based
    insight = fallback_insight(metric)
    insight['source'] = 'fallback'
    return insight
```

**Monitor Fallback Rate:**
```python
import boto3

cloudwatch = boto3.client('cloudwatch')

def record_insight_source(source: str) -> None:
    """Track AI vs fallback usage."""
    cloudwatch.put_metric_data(
        Namespace='IOpsDashboard',
        MetricData=[{
            'MetricName': 'InsightSource',
            'Value': 1,
            'Unit': 'Count',
            'Dimensions': [{'Name': 'Source', 'Value': source}],
        }]
    )
```

### 6.4 Data Comparison (Simulated vs Real AI)

**Comparison Script:**
```python
# tests/comparison/compare_insights.py

def compare_insights(metric: dict) -> dict:
    """
    Generate both simulated and AI insights for comparison.

    Use during canary phase to validate AI quality.
    """
    # Generate simulated insight (current test script logic)
    simulated = generate_simulated_insight(metric)

    # Generate real AI insight
    ai = generate_insight(metric)

    comparison = {
        'metric': metric,
        'simulated': simulated,
        'ai': ai,
        'differences': {
            'prediction_type_match': simulated['prediction_type'] == ai['prediction_type'],
            'risk_score_delta': abs(simulated['risk_score'] - ai['risk_score']),
            'explanation_length_ratio': len(ai['explanation']) / len(simulated['explanation']),
        },
    }

    # Log for analysis
    print(json.dumps(comparison, indent=2))

    return comparison

def generate_simulated_insight(metric: dict) -> dict:
    """Original test script logic."""
    if metric['utilization'] > 90:
        return {
            'prediction_type': 'resource_saturation',
            'risk_score': 85,
            'explanation': f"Port utilization at {metric['utilization']}%",
            'recommendations': ['Scale workload', 'Check for hot spots'],
        }
    # ... other rules
```

**Analysis Dashboard:**
```python
def analyze_comparison_results(results: List[dict]) -> None:
    """Analyze simulated vs AI insight quality."""
    matches = sum(1 for r in results if r['differences']['prediction_type_match'])
    avg_risk_delta = statistics.mean([r['differences']['risk_score_delta'] for r in results])

    print(f"\n=== Insight Comparison Analysis ===")
    print(f"Total comparisons: {len(results)}")
    print(f"Prediction type matches: {matches} ({matches/len(results)*100:.1f}%)")
    print(f"Average risk score delta: {avg_risk_delta:.1f} points")
    print(f"\nAI insights are {'MORE' if avg_risk_delta > 10 else 'SIMILARLY'} sensitive")
```

---

## 7. Cost Analysis

### 7.1 Bedrock Pricing (Claude 3.5 Haiku)

**Pricing (us-east-2):**
- **Input tokens:** $0.25 per 1M tokens
- **Output tokens:** $1.25 per 1M tokens

**Per Insight Costs:**
```
Average prompt size: 500 tokens (metric + context)
Average output size: 200 tokens (JSON response)

Input cost:  500 tokens × $0.25 / 1M = $0.000125
Output cost: 200 tokens × $1.25 / 1M = $0.000250
Total per insight: $0.000375
```

**Volume Projections:**

| Insights/Day | Daily Cost | Monthly Cost | Annual Cost |
|--------------|------------|--------------|-------------|
| 100 | $0.04 | $1.13 | $13.69 |
| 1,000 | $0.38 | $11.25 | $136.88 |
| 10,000 | $3.75 | $112.50 | $1,368.75 |
| 50,000 | $18.75 | $562.50 | $6,843.75 |
| 100,000 | $37.50 | $1,125.00 | $13,687.50 |

### 7.2 Supporting Infrastructure Costs

**Lambda Execution:**
- Process Lambda: 128MB, 10ms avg → $0.0000002 per invocation
- AI Lambda: 512MB, 500ms avg → $0.000001 per invocation
- Monthly (100k insights): ~$150

**DynamoDB:**
- Additional writes: 100k/day = 3M/month → $3.75
- Reads (unchanged): $5/month
- Total: ~$9/month

**EventBridge:**
- Rule evaluations: Free (first 14M)
- SNS notifications: $0.50 per 1M → $0.05/month (10k critical alerts)

**Total Infrastructure (100k insights/month):**
- Bedrock: $1,125
- Lambda: $150
- DynamoDB: $9
- Other: $1
- **Total: ~$1,285/month**

### 7.3 Cost Optimization Strategies

**1. Intelligent Triggering**
```python
def should_trigger_ai_analysis(metric: dict) -> bool:
    """
    Only trigger AI for metrics likely to be actionable.

    Avoids AI inference for:
    - Normal metrics (utilization 40-70%, latency < 2x baseline)
    - Recent duplicates (same stream + similar values)
    - Low-confidence signals
    """
    # Skip normal metrics
    if (
        40 <= metric['utilization'] <= 70 and
        metric['latency_p99'] < metric['baseline_latency'] * 2 and
        metric['error_rate'] < 0.01
    ):
        return False

    # Skip recent duplicates (cache last 5min of insights per stream)
    recent_key = f"{metric['stream_id']}:{metric['prediction_type']}"
    if recent_key in recent_insights_cache:
        return False

    return True
```

**Estimated Reduction:** 60-70% fewer AI calls

**2. Batch Processing**
```python
def process_batch_with_ai(metrics: List[dict]) -> List[dict]:
    """
    Process multiple metrics in single Bedrock call.

    Saves:
    - Network overhead
    - Per-call latency
    - ~20% tokens vs individual calls
    """
    batch_prompt = f"""
    Analyze the following {len(metrics)} InfiniBand metrics.
    Return a JSON array with one insight per metric.

    Metrics:
    {json.dumps(metrics, indent=2)}

    Output format:
    [
      {{"metric_index": 0, "prediction_type": "...", ...}},
      {{"metric_index": 1, "prediction_type": "...", ...}}
    ]
    """

    response = bedrock_client.invoke_model(...)
    insights = parse_batch_response(response)
    return insights
```

**Estimated Reduction:** 15-20% cost savings

**3. Tiered Analysis**
```python
def get_analysis_tier(metric: dict) -> str:
    """
    Use different prompt complexity based on severity.

    - Critical: Full context + examples (600 tokens)
    - High: Standard context (400 tokens)
    - Medium: Basic analysis (200 tokens)
    """
    if metric['risk_score'] >= 80:
        return 'critical'
    elif metric['risk_score'] >= 60:
        return 'high'
    else:
        return 'medium'

PROMPTS = {
    'critical': """[Full detailed prompt with examples]""",
    'high': """[Standard prompt]""",
    'medium': """[Minimal prompt]""",
}
```

**Estimated Reduction:** 25-30% token usage

**4. Caching and Deduplication**
```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
def get_cached_insight(metric_hash: str) -> dict:
    """
    Cache AI insights for similar metrics.

    Use hash of key metric values as cache key.
    """
    return dynamodb_query_recent_insight(metric_hash)

def generate_or_retrieve_insight(metric: dict) -> dict:
    """Check cache before calling Bedrock."""
    metric_hash = hashlib.md5(json.dumps({
        'stream_id': metric['stream_id'],
        'prediction_type': classify_metric(metric),
        'risk_bucket': metric['utilization'] // 10,
    }).encode()).hexdigest()

    cached = get_cached_insight(metric_hash)
    if cached:
        return cached

    insight = generate_insight(metric)
    cache_insight(metric_hash, insight)
    return insight
```

**Estimated Reduction:** 30-40% duplicate calls

**Combined Savings:** 70-80% cost reduction possible

### 7.4 Budget Monitoring and Alerts

**CloudWatch Cost Alarm:**
```typescript
const bedrockCostAlarm = new cloudwatch.Alarm(this, 'BedrockCostAlarm', {
  metric: new cloudwatch.Metric({
    namespace: 'AWS/Bedrock',
    metricName: 'InvokedModelCharges',
    dimensionsMap: {
      ModelId: 'anthropic.claude-3-5-haiku-20241022-v1:0',
    },
    statistic: 'Sum',
    period: Duration.hours(1),
  }),
  threshold: 10,  // $10/hour threshold
  evaluationPeriods: 1,
  comparisonOperator: cloudwatch.ComparisonOperator.GREATER_THAN_THRESHOLD,
  treatMissingData: cloudwatch.TreatMissingData.NOT_BREACHING,
});

bedrockCostAlarm.addAlarmAction(new actions.SnsAction(alertTopic));
```

**Daily Cost Report Lambda:**
```python
import boto3
from datetime import datetime, timedelta

ce = boto3.client('ce')  # Cost Explorer

def get_daily_bedrock_cost():
    """Query actual Bedrock spend."""
    end = datetime.now().date()
    start = end - timedelta(days=1)

    response = ce.get_cost_and_usage(
        TimePeriod={
            'Start': start.isoformat(),
            'End': end.isoformat(),
        },
        Granularity='DAILY',
        Filter={
            'Dimensions': {
                'Key': 'SERVICE',
                'Values': ['Amazon Bedrock'],
            },
        },
        Metrics=['UnblendedCost'],
    )

    cost = float(response['ResultsByTime'][0]['Total']['UnblendedCost']['Amount'])

    # Send to CloudWatch + SNS
    cloudwatch.put_metric_data(
        Namespace='IOpsDashboard',
        MetricData=[{
            'MetricName': 'BedrockDailyCost',
            'Value': cost,
            'Unit': 'None',
            'Timestamp': datetime.now(),
        }]
    )

    return cost
```

**Budget Configuration:**
```bash
aws budgets create-budget \
  --account-id 971422717446 \
  --budget '{
    "BudgetName": "IOPS-Bedrock-Monthly",
    "BudgetLimit": {
      "Amount": "500",
      "Unit": "USD"
    },
    "TimeUnit": "MONTHLY",
    "BudgetType": "COST",
    "CostFilters": {
      "Service": ["Amazon Bedrock"]
    }
  }' \
  --notifications-with-subscribers '[
    {
      "Notification": {
        "NotificationType": "ACTUAL",
        "ComparisonOperator": "GREATER_THAN",
        "Threshold": 80,
        "ThresholdType": "PERCENTAGE"
      },
      "Subscribers": [{
        "SubscriptionType": "EMAIL",
        "Address": "alerts@example.com"
      }]
    }
  ]'
```

---

## 8. Implementation Checklist

### Phase 1: Infrastructure Setup (2-3 days)

- [ ] **IAM Roles**
  - [ ] Update Process Lambda role with Lambda invoke permission
  - [ ] Update AI Lambda role with Bedrock permissions
  - [ ] Add EventBridge PutEvents permission to AI Lambda
  - [ ] Verify GSI access for all Lambdas

- [ ] **EventBridge Configuration**
  - [ ] Create CriticalAlertRule with risk_score >= 80 filter
  - [ ] Create SNS topic for critical alerts
  - [ ] Subscribe email addresses to SNS topic
  - [ ] Add CloudWatch Logs target for audit trail
  - [ ] Test EventBridge rule with manual event

- [ ] **CloudWatch Alarms**
  - [ ] Create AI Lambda error alarm
  - [ ] Create Bedrock throttling alarm
  - [ ] Create daily cost alarm
  - [ ] Set up SNS notifications for all alarms

- [ ] **Kinesis Event Source**
  - [ ] Add EventSourceMapping to Process Lambda
  - [ ] Configure batch size (start with 100)
  - [ ] Set up DLQ for failed batches
  - [ ] Enable bisectBatchOnError

### Phase 2: Code Implementation (3-4 days)

- [ ] **Process Lambda Updates**
  - [ ] Implement `should_trigger_ai_analysis()` logic
  - [ ] Add baseline calculation from DynamoDB
  - [ ] Implement anomaly detection
  - [ ] Add Lambda invocation for AI function
  - [ ] Implement batch processing
  - [ ] Add AI_ENABLED and AI_SAMPLING_RATE env vars
  - [ ] Write unit tests (90% coverage target)

- [ ] **AI Lambda Enhancements**
  - [ ] Update handler to write to DynamoDB
  - [ ] Implement batch metric processing
  - [ ] Build comprehensive InfiniBand prompts
  - [ ] Add response parsing with fallback
  - [ ] Implement retry logic with exponential backoff
  - [ ] Add rate limiter for Bedrock calls
  - [ ] Implement EventBridge alert triggering
  - [ ] Add fallback_insight() for errors
  - [ ] Write unit tests (95% coverage target)

- [ ] **Testing**
  - [ ] Unit tests for Process Lambda
  - [ ] Unit tests for AI Lambda
  - [ ] Mock Bedrock client in tests
  - [ ] Integration test: Kinesis → Insight
  - [ ] Load test: 100 concurrent requests
  - [ ] Verify fallback behavior

### Phase 3: CDK/Infrastructure as Code (1-2 days)

- [ ] **Update `cdk/lib/cdk-stack.ts`**
  - [ ] Add Kinesis EventSourceMapping
  - [ ] Add EventBridge rule and targets
  - [ ] Add SNS topic and subscriptions
  - [ ] Update IAM roles with new permissions
  - [ ] Add CloudWatch alarms
  - [ ] Add environment variables for feature flags
  - [ ] Synthesize and validate stack

- [ ] **Deployment**
  - [ ] Deploy to dev environment
  - [ ] Verify all resources created
  - [ ] Test manually with AI_ENABLED=false
  - [ ] Smoke test with AI_ENABLED=true

### Phase 4: Testing & Validation (2-3 days)

- [ ] **Integration Testing**
  - [ ] Send test event to Kinesis
  - [ ] Verify Process Lambda triggers
  - [ ] Verify AI Lambda invoked
  - [ ] Check insight written to DynamoDB
  - [ ] Verify EventBridge alert for critical (risk >= 80)
  - [ ] Check SNS email received
  - [ ] Query insight via dashboard API

- [ ] **Load Testing**
  - [ ] Run 100 events through pipeline
  - [ ] Monitor Bedrock throttling
  - [ ] Check fallback rate
  - [ ] Verify all insights stored
  - [ ] Measure end-to-end latency
  - [ ] Check cost estimates vs actuals

- [ ] **Quality Validation**
  - [ ] Compare AI vs simulated insights (10 samples)
  - [ ] Validate prediction type accuracy
  - [ ] Check risk score appropriateness
  - [ ] Review explanation quality
  - [ ] Verify recommendations actionable

### Phase 5: Gradual Rollout (2-3 weeks)

- [ ] **Week 1: Canary (10%)**
  - [ ] Set `AI_SAMPLING_RATE=0.1`
  - [ ] Deploy to production
  - [ ] Monitor for 2-3 days
  - [ ] Check error rates and costs
  - [ ] Gather initial feedback

- [ ] **Week 2: Ramp Up**
  - [ ] Increase to 25% (monitor 2 days)
  - [ ] Increase to 50% (monitor 2 days)
  - [ ] Increase to 75% (monitor 2 days)
  - [ ] Tune prompts based on real data

- [ ] **Week 3: Full Rollout**
  - [ ] Set `AI_SAMPLING_RATE=1.0`
  - [ ] Monitor for 48 hours
  - [ ] Verify all insights AI-generated
  - [ ] Disable test script simulation mode
  - [ ] Update documentation

### Phase 6: Monitoring & Optimization (Ongoing)

- [ ] **Cost Monitoring**
  - [ ] Set up AWS Budget alert ($500/month)
  - [ ] Create daily cost report Lambda
  - [ ] Implement cost dashboard
  - [ ] Review weekly cost trends

- [ ] **Performance Optimization**
  - [ ] Implement intelligent triggering (reduce 60%)
  - [ ] Add batch processing (reduce 20%)
  - [ ] Implement caching (reduce 30%)
  - [ ] Tune prompts for token efficiency

- [ ] **Quality Improvements**
  - [ ] Collect false positive/negative data
  - [ ] Refine risk score thresholds
  - [ ] Add domain-specific examples to prompts
  - [ ] Implement feedback loop for continuous improvement

---

## 9. Success Criteria

### Technical Metrics

**Reliability:**
- ✅ 99.9% AI Lambda success rate
- ✅ <5% fallback usage (AI failures)
- ✅ Zero data loss (all metrics processed)

**Performance:**
- ✅ <30s latency from Kinesis to insight in DynamoDB
- ✅ <2s Bedrock API P95 latency
- ✅ Handle 100+ events/minute sustained

**Quality:**
- ✅ >90% prediction type accuracy vs expert review
- ✅ Risk scores within ±10 points of manual assessment
- ✅ >85% of recommendations rated "actionable"

### Business Metrics

**Cost:**
- ✅ Stay within $500/month budget
- ✅ <$0.01 cost per insight
- ✅ 50%+ cost reduction via optimizations

**User Value:**
- ✅ Insights actionable without context
- ✅ <5% false positive rate for critical alerts
- ✅ Explanations understandable to network operators

**Operational:**
- ✅ Zero manual intervention required
- ✅ Alerts arrive within 60s of detection
- ✅ 24/7 autonomous operation

---

## 10. Risks and Mitigations

### Risk 1: Bedrock API Throttling

**Likelihood:** Medium
**Impact:** High

**Scenario:** Exceed 2,000 RPM quota during traffic spikes

**Mitigation:**
1. Implement rate limiter (1,800 RPM hard cap)
2. Use exponential backoff retry
3. Batch processing to reduce call count
4. Request quota increase via AWS Support (preventive)
5. Fallback to rule-based insights

**Detection:** CloudWatch alarm on ThrottlingException

### Risk 2: Bedrock Cost Overrun

**Likelihood:** Medium
**Impact:** High

**Scenario:** Unexpected traffic spike → 10x costs

**Mitigation:**
1. AWS Budget alert at 80% of $500/month
2. Intelligent triggering (60% reduction)
3. CloudWatch alarm at $10/hour
4. Feature flag to disable AI if needed
5. Cost dashboard for real-time monitoring

**Detection:** Daily cost report + budget alerts

### Risk 3: Poor Insight Quality

**Likelihood:** Medium
**Impact:** Medium

**Scenario:** AI generates irrelevant or inaccurate insights

**Mitigation:**
1. Start with 10% sampling (canary)
2. Compare AI vs simulated insights
3. Iterative prompt tuning
4. Fallback for low confidence (<0.7)
5. User feedback mechanism

**Detection:** Manual review during canary phase

### Risk 4: Kinesis Processing Lag

**Likelihood:** Low
**Impact:** High

**Scenario:** Process Lambda can't keep up with event volume

**Mitigation:**
1. Auto-scaling (Kinesis handles this)
2. Increase batch size (100 → 500)
3. Optimize Lambda memory/timeout
4. Add parallel shards if needed
5. CloudWatch alarm on IteratorAge

**Detection:** CloudWatch IteratorAge metric

### Risk 5: DynamoDB Hot Partition

**Likelihood:** Low
**Impact:** Medium

**Scenario:** High write volume to single partition causes throttling

**Mitigation:**
1. Use unique entity_id per insight
2. Batch writes (25 items per call)
3. DynamoDB on-demand scaling
4. Monitor for WriteThrottleEvents

**Detection:** CloudWatch WriteThrottleEvents metric

### Risk 6: EventBridge Alert Overload

**Likelihood:** Medium
**Impact:** Low

**Scenario:** Too many critical alerts (risk >= 80) → email spam

**Mitigation:**
1. Tune risk score thresholds during canary
2. Implement alert deduplication (5-min window)
3. Add SNS filter policy for specific types
4. Create alert digest (hourly summary)

**Detection:** SNS delivery metrics

---

## 11. Rollback Plan

### Trigger Conditions

Roll back immediately if:
- AI Lambda error rate > 10%
- Bedrock cost > $50/day
- >50% insights using fallback
- Critical alerts false positive rate > 20%

### Rollback Steps

**Step 1: Disable AI (5 minutes)**
```bash
# Update Lambda environment variable
aws lambda update-function-configuration \
  --function-name IOpsDashboard-ProcessFunction \
  --environment Variables="{AI_ENABLED=false}" \
  --region us-east-2
```

**Step 2: Verify Rollback (10 minutes)**
- Monitor Process Lambda logs (AI invocations should stop)
- Check Bedrock costs stop increasing
- Verify dashboard still receives data (from existing flow)

**Step 3: Root Cause Analysis (1-2 hours)**
- Review CloudWatch Logs for errors
- Check Bedrock API responses
- Analyze insight quality issues
- Review cost breakdown

**Step 4: Fix and Re-Deploy (varies)**
- Fix identified issues
- Test in dev environment
- Re-deploy with AI_ENABLED=false
- Gradually re-enable with lower sampling rate

### Zero-Impact Rollback

**Key Design:** AI is additive, not replacement
- Existing metrics still flow: Kinesis → Process → DynamoDB
- Dashboard API unchanged
- Test script can still generate data
- No data loss or service disruption

---

## 12. Documentation Updates

### Files to Update

**1. README.md**
- Add "AI-Powered Insights" section
- Update architecture diagram
- Note Bedrock usage and costs

**2. docs/ARCHITECTURE.md** (new)
```markdown
# IOPS Dashboard Architecture

## Data Flow with AI Inference

Kinesis Stream → Process Lambda → DynamoDB → AI Lambda → DynamoDB
                 (Aggregate)       (Metrics)    (Bedrock)   (Insights)
                                                   ↓
                                              EventBridge → SNS
```

**3. docs/AI-INTEGRATION.md** (new)
- Bedrock configuration
- Prompt engineering guide
- Cost monitoring
- Troubleshooting

**4. docs/DEPLOYMENT-STATUS.md**
- Update with AI Lambda status
- Note EventBridge rules
- List SNS subscriptions

**5. docs/COST-ANALYSIS.md** (new)
- Bedrock pricing breakdown
- Monthly cost projections
- Optimization strategies

### Code Comments

Add inline documentation:
```python
def should_trigger_ai_analysis(metric: dict) -> bool:
    """
    Determine if metric requires AI inference.

    This function implements intelligent triggering to reduce costs
    by only analyzing metrics that are likely actionable. Normal
    operating conditions are filtered out.

    Args:
        metric: Enriched metric with baseline comparisons

    Returns:
        True if AI analysis should be triggered

    Cost Impact:
        Reduces AI calls by ~60% while maintaining coverage of
        all critical issues (utilization >80%, high error rates)
    """
```

---

## 13. Timeline Estimate

### Detailed Timeline

**Week 1: Infrastructure & Code**
- Days 1-2: IAM roles, EventBridge, SNS, CloudWatch alarms
- Days 3-4: Process Lambda pattern detection
- Days 5: AI Lambda DynamoDB writes + EventBridge

**Week 2: Testing**
- Days 1-2: Unit tests for both Lambdas
- Day 3: Integration testing (Kinesis → Insight)
- Days 4-5: Load testing and optimization

**Week 3: Deployment & Canary**
- Day 1: Deploy with AI_ENABLED=false
- Days 2-7: Canary at 10% sampling, monitor closely

**Week 4: Ramp Up**
- Days 1-2: Increase to 25%
- Days 3-4: Increase to 50%
- Days 5-7: Increase to 75%, tune prompts

**Week 5: Full Rollout**
- Days 1-2: Enable 100%, monitor intensely
- Days 3-5: Quality validation and optimization
- Days 6-7: Documentation and handoff

**Total: 5 weeks (including careful rollout)**

### Fast-Track Option (2 weeks)

If aggressive timeline needed:
- Week 1: Infrastructure + Code + Tests (compress testing)
- Week 2: Deploy directly to 50% → 100% (skip canary phases)

**Risk:** Higher chance of issues in production

---

## 14. Post-Implementation

### Metrics Dashboard

Create CloudWatch dashboard with:
- AI Lambda invocations/minute
- Bedrock API latency (P50, P95, P99)
- Fallback rate (AI failures)
- Daily Bedrock cost
- Insight generation rate
- EventBridge critical alerts/hour
- Prediction type distribution

### Weekly Review

Schedule weekly review of:
- Cost trends vs budget
- Insight quality feedback
- False positive/negative rate
- Optimization opportunities
- Prompt improvements needed

### Continuous Improvement

**Month 1:**
- Baseline metrics collection
- Identify top improvement areas
- User feedback gathering

**Month 2:**
- Implement optimizations (batching, caching)
- Tune risk score thresholds
- Refine prompts with examples

**Month 3+:**
- Cost reduction initiatives (target 50%)
- Quality improvements (target <5% false positives)
- Explore advanced Bedrock features (Claude 4)

---

## Summary

This PR will enable real AI inference in the IOPS Dashboard using AWS Bedrock (Claude 3.5 Haiku) for InfiniBand monitoring insights. The implementation includes:

✅ **Kinesis → Process → AI → DynamoDB pipeline**
✅ **EventBridge alerts for critical insights**
✅ **Comprehensive error handling and fallbacks**
✅ **Cost optimization strategies (70-80% reduction)**
✅ **Gradual rollout with feature flags**
✅ **Extensive testing (unit, integration, load)**

**Estimated effort:** 3-5 weeks with careful rollout
**Estimated cost:** $100-500/month depending on volume
**Success criteria:** >99.9% reliability, <$0.01/insight, >90% accuracy

**Next Steps:**
1. Review and approve this plan
2. Create GitHub issue/ticket
3. Begin Phase 1 (Infrastructure Setup)
4. Weekly progress reviews

---

**Document Version:** 1.0
**Created:** November 5, 2025
**Author:** Claude Code
**Status:** Ready for Review
